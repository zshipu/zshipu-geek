<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>BERT on 知识铺的博客</title>
    <link>https://geek.zshipu.com/tags/BERT/</link>
    <description>Recent content in BERT on 知识铺的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 15 Mar 2022 11:32:45 +0800</lastBuildDate>
    <atom:link href="https://geek.zshipu.com/tags/BERT/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>的优秀变体论文图解介绍</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%9A%84%E4%BC%98%E7%A7%80%E5%8F%98%E4%BD%93%E8%AE%BA%E6%96%87%E5%9B%BE%E8%A7%A3%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Tue, 15 Mar 2022 11:32:45 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%9A%84%E4%BC%98%E7%A7%80%E5%8F%98%E4%BD%93%E8%AE%BA%E6%96%87%E5%9B%BE%E8%A7%A3%E4%BB%8B%E7%BB%8D/</guid>
      <description>作者：amitness 编译：ronghuaiyang 导读: ALBERT作为BERT的一个变体，在保持性能的基础上，大大减少了模型的参数，使得实用变得更加方便，是经典的BERT变体之一。 BERT 的 youxiu 变体：ALBERT 论文图解介绍 考虑下面给出的句子。作为人类，当我们遇到“ apple”这个词</description>
    </item>
    <item>
      <title>用实现可扩展快速且高效的部署</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%94%A8%E5%AE%9E%E7%8E%B0%E5%8F%AF%E6%89%A9%E5%B1%95%E5%BF%AB%E9%80%9F%E4%B8%94%E9%AB%98%E6%95%88%E7%9A%84%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Tue, 15 Mar 2022 11:21:01 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%94%A8%E5%AE%9E%E7%8E%B0%E5%8F%AF%E6%89%A9%E5%B1%95%E5%BF%AB%E9%80%9F%E4%B8%94%E9%AB%98%E6%95%88%E7%9A%84%E9%83%A8%E7%BD%B2/</guid>
      <description>文 / 由特邀作者 SAP Concur Labs 的高级机器学习工程师 Hannes Hapke 发布。由 Robert Crowe 代表 TFX 团队编辑。 Transformer 模型（尤其是 BERT 模型）为 NLP 带来巨大的变革，并且在情感分析、实体提取和问答问题等任务的处理上也均有新的突破。BERT 模型让数据科学家站在了巨人的肩膀上。各公司已经通过大型语料库对模型进行预训练，数据科学家可以对</description>
    </item>
    <item>
      <title>基于的纠错</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%9F%BA%E4%BA%8E%E7%9A%84%E7%BA%A0%E9%94%99/</link>
      <pubDate>Tue, 15 Mar 2022 10:38:35 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%9F%BA%E4%BA%8E%E7%9A%84%E7%BA%A0%E9%94%99/</guid>
      <description>分享嘉宾：魏天闻 小米人工智能部 编辑整理：李淑娜 内容来源：DataFunTalk 导读： 小爱同学是小米公司开发的智能语音系统，已广泛应用在手机、手环、音箱、电视等电子产品中，并支持闲聊、问答、语音控制等多种语音交互场景。语音系统中语音内容识别 ( ASR ) 的精准性，是影响智能语音产品发展的关</description>
    </item>
    <item>
      <title>美团搜索中查询改写技术的探索与实践</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%BE%8E%E5%9B%A2%E6%90%9C%E7%B4%A2%E4%B8%AD%E6%9F%A5%E8%AF%A2%E6%94%B9%E5%86%99%E6%8A%80%E6%9C%AF%E7%9A%84%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Tue, 15 Mar 2022 10:06:10 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%BE%8E%E5%9B%A2%E6%90%9C%E7%B4%A2%E4%B8%AD%E6%9F%A5%E8%AF%A2%E6%94%B9%E5%86%99%E6%8A%80%E6%9C%AF%E7%9A%84%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%AE%9E%E8%B7%B5/</guid>
      <description>杨俭 宗宇 谢睿等 美团技术团队 稿 1. 引言 在搜索场景中，由于用户搜索词Query和检索文本Document之间存在大量表述不一的情况，在文本检索框架下，此类文本不匹配导致的漏召回问题严重影响着用户的体验。对这类问题业界一般有两种方案：用户端拓展用户的查询词——即查询改写，或Documen</description>
    </item>
    <item>
      <title>时代的创新应用篇在各领域的应用进展</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E6%97%B6%E4%BB%A3%E7%9A%84%E5%88%9B%E6%96%B0%E5%BA%94%E7%94%A8%E7%AF%87%E5%9C%A8%E5%90%84%E9%A2%86%E5%9F%9F%E7%9A%84%E5%BA%94%E7%94%A8%E8%BF%9B%E5%B1%95/</link>
      <pubDate>Tue, 15 Mar 2022 09:55:22 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E6%97%B6%E4%BB%A3%E7%9A%84%E5%88%9B%E6%96%B0%E5%BA%94%E7%94%A8%E7%AF%87%E5%9C%A8%E5%90%84%E9%A2%86%E5%9F%9F%E7%9A%84%E5%BA%94%E7%94%A8%E8%BF%9B%E5%B1%95/</guid>
      <description>Bert 给人们带来了大惊喜，不过转眼过去大约半年时间了，这半年来，陆续出现了与Bert相关的不少新工作。 最近几个月，在主业做推荐算法之外的时间，我其实一直比较好奇下面两个问题： 问题一：Bert原始的论文证明了：在GLUE这种综合的NLP数据集合下，Bert预训练对几乎所有类型的NLP任</description>
    </item>
    <item>
      <title>在美团搜索核心排序的探索和实践</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%9C%A8%E7%BE%8E%E5%9B%A2%E6%90%9C%E7%B4%A2%E6%A0%B8%E5%BF%83%E6%8E%92%E5%BA%8F%E7%9A%84%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Tue, 15 Mar 2022 09:34:48 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%9C%A8%E7%BE%8E%E5%9B%A2%E6%90%9C%E7%B4%A2%E6%A0%B8%E5%BF%83%E6%8E%92%E5%BA%8F%E7%9A%84%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%AE%9E%E8%B7%B5/</guid>
      <description>美团点评技术团队 引言 美团搜索是美团App上最大的连接人和服务的入口，覆盖了团购、外卖、电影、酒店、买菜等各种生活服务。随着用户量快速增长，越来越多的用户在不同场景下都会通过搜索来获取自己想要的服务。理解用户Query，将用户最想要的结果排在靠前的位置，是搜索引擎最核心的两大步骤。</description>
    </item>
    <item>
      <title>深度语义模型在同城搜索的实践</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%90%8C%E5%9F%8E%E6%90%9C%E7%B4%A2%E7%9A%84%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Tue, 15 Mar 2022 09:25:23 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%90%8C%E5%9F%8E%E6%90%9C%E7%B4%A2%E7%9A%84%E5%AE%9E%E8%B7%B5/</guid>
      <description>分享嘉宾: 熊威，58同城TEG搜索排序部资深算法工程师 整理出品: 张劲， AICUG人工智能社区 PPT下载： http://www.aicug.cn/#/docs 浏览器不支持该媒体的播放 :( （视频回放） 导读 传统基于Term-Match检索技术可以较好的解决Query-Doc字面匹配问题，而对于没有词命中时的Query-Doc语义匹配问题</description>
    </item>
    <item>
      <title>谈一谈中语言模型的发展</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%B0%88%E4%B8%80%E8%B0%88%E4%B8%AD%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%91%E5%B1%95/</link>
      <pubDate>Tue, 15 Mar 2022 09:19:02 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%B0%88%E4%B8%80%E8%B0%88%E4%B8%AD%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%91%E5%B1%95/</guid>
      <description>作者：章立 美团算法工程师 未经作者允许 严禁转载！ 问题定义 一段文字，例如：今夜月色真美。代表的是什么含义？如果在春天温度适宜的 9、10 点站在阳台的人对你脱口而出地说出这句话，你会怎么理解这句话，亦或者你会怎么回应他(她)呢？ 这句话是十九世纪末的文学家 夏目漱石对 I love you 的英译日标注结果（</description>
    </item>
    <item>
      <title>遇上使用和构建搜索引擎</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E9%81%87%E4%B8%8A%E4%BD%BF%E7%94%A8%E5%92%8C%E6%9E%84%E5%BB%BA%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/</link>
      <pubDate>Tue, 15 Mar 2022 09:14:06 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E9%81%87%E4%B8%8A%E4%BD%BF%E7%94%A8%E5%92%8C%E6%9E%84%E5%BB%BA%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/</guid>
      <description>作者：Hironsan 编译：ronghuaiyang 导读： 强强联合，看看是否能有1+1＞2的效果。 在这篇文章中，我们使用一个预先训练好的BERT模型和Elasticsearch来构建一个搜索引擎。Elasticsearch最近发布了带有矢量字段的文本相似性搜索。另一方面，你可以使</description>
    </item>
    <item>
      <title>从到模型自然语言处理中的预训练技术发展史</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E4%BB%8E%E5%88%B0%E6%A8%A1%E5%9E%8B%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2/</link>
      <pubDate>Tue, 15 Mar 2022 09:08:40 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E4%BB%8E%E5%88%B0%E6%A8%A1%E5%9E%8B%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2/</guid>
      <description>Bert最近很火，应该是最近最火爆的AI进展，网上的评价很高，那么Bert值得这么高的评价吗？我个人判断是值得。那为什么会有这么高的评价呢？是因为它有重大的理论或者模型创新吗？其实并没有，从模型创新角度看一般，创新不算大。但是架不住效果太好了，基本刷新了很多NLP的任务的最好性能</description>
    </item>
    <item>
      <title>关键短语抽取及使用的技术实践</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%85%B3%E9%94%AE%E7%9F%AD%E8%AF%AD%E6%8A%BD%E5%8F%96%E5%8F%8A%E4%BD%BF%E7%94%A8%E7%9A%84%E6%8A%80%E6%9C%AF%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Tue, 15 Mar 2022 09:08:33 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%85%B3%E9%94%AE%E7%9F%AD%E8%AF%AD%E6%8A%BD%E5%8F%96%E5%8F%8A%E4%BD%BF%E7%94%A8%E7%9A%84%E6%8A%80%E6%9C%AF%E5%AE%9E%E8%B7%B5/</guid>
      <description>AI 算法团队 zacbai 平安寿险 PAI 一、 全文框架概览 一、关键短语抽取简介 关键短语抽取 (keyphrase extraction)，指从文章中提取典型的、有代表性的短语，期望能够表达文章的关键内容。 关键短语抽取对于文章理解、搜索、分类、聚类都很重要。而高质量的关键短语抽取算法，还能有效助力构建知识图谱。 常见的关键短</description>
    </item>
    <item>
      <title>贝壳找房语言模型系列原理篇二从到</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%B3%BB%E5%88%97%E5%8E%9F%E7%90%86%E7%AF%87%E4%BA%8C%E4%BB%8E%E5%88%B0/</link>
      <pubDate>Tue, 15 Mar 2022 09:06:46 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%B3%BB%E5%88%97%E5%8E%9F%E7%90%86%E7%AF%87%E4%BA%8C%E4%BB%8E%E5%88%B0/</guid>
      <description>上一篇 贝壳找房【语言模型系列】原理篇一：从 one-hot 到 Word2vec 讲到了 word2vec 存在”一词多义“的问题，其主要原因在于 word2vec 生成的词向量是“静态”的，每一个词固定的对应着一个词向量表示，也就是说在 word2vec 训练好之后，在使用单词的向量表示的时候，不论该词的上下文是什么，这个单词的向量表示不会随着上下文语境的变化而改</description>
    </item>
    <item>
      <title>的嵌入层是如何实现的看完你就明白了</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%9A%84%E5%B5%8C%E5%85%A5%E5%B1%82%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%9A%84%E7%9C%8B%E5%AE%8C%E4%BD%A0%E5%B0%B1%E6%98%8E%E7%99%BD%E4%BA%86/</link>
      <pubDate>Tue, 15 Mar 2022 09:06:34 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%9A%84%E5%B5%8C%E5%85%A5%E5%B1%82%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%9A%84%E7%9C%8B%E5%AE%8C%E4%BD%A0%E5%B0%B1%E6%98%8E%E7%99%BD%E4%BA%86/</guid>
      <description>作者：__ 编译：ronghuaiyang 导读： 非常简单直白的语言解释了BERT中的嵌入层的组成以及实现的方式。 介绍 在本文中，我将解释BERT中嵌入层的实现细节，即 token 嵌入、Segment 嵌入和 Position 嵌入。 简介 这是一张来自论文的图，它恰当地描述了 BERT 中每一个嵌入层的功能： 与大多数旨在解决 nlp</description>
    </item>
    <item>
      <title>新秀的优雅解读</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E6%96%B0%E7%A7%80%E7%9A%84%E4%BC%98%E9%9B%85%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Tue, 15 Mar 2022 09:05:46 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E6%96%B0%E7%A7%80%E7%9A%84%E4%BC%98%E9%9B%85%E8%A7%A3%E8%AF%BB/</guid>
      <description>作为2018年自然语言处理领域的新秀，BERT做到了过去几年NLP重大进展的集大成，一出场就技惊四座碾压竞争对手，刷新了11项NLP测试的最高纪录，甚至超越了人类的表现，相信会是未来NLP研究和工业应用最主流的语言模型之一。本文尝试由浅入深，为各位看客带来优雅的BERT解读。 NL</description>
    </item>
    <item>
      <title>一文读懂深度学习从神经元到</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%85%83%E5%88%B0/</link>
      <pubDate>Tue, 15 Mar 2022 09:05:45 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%85%83%E5%88%B0/</guid>
      <description>作者： \* 世恩、风引、调参 一个神经网络结构通常包含输入层、隐藏层、输出层。输入层是我们的 features (特征)，输出层是我们的预测 (prediction)。神经网络的目的是拟合一个函数 f*：features -&amp;gt; prediction。在训练期间，通过减小 prediction 和实际 label 的差异的这种方式，来更改网络参数，</description>
    </item>
    <item>
      <title>大火却不懂读这一篇就够了</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%A4%A7%E7%81%AB%E5%8D%B4%E4%B8%8D%E6%87%82%E8%AF%BB%E8%BF%99%E4%B8%80%E7%AF%87%E5%B0%B1%E5%A4%9F%E4%BA%86/</link>
      <pubDate>Tue, 15 Mar 2022 09:04:42 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%A4%A7%E7%81%AB%E5%8D%B4%E4%B8%8D%E6%87%82%E8%AF%BB%E8%BF%99%E4%B8%80%E7%AF%87%E5%B0%B1%E5%A4%9F%E4%BA%86/</guid>
      <description>大数据文摘与百度NLP联合出品 编译：张驰、毅航、Conrad、龙心尘 编者按：前一段时间谷歌推出的BERT模型在11项NLP任务中夺得STOA结果，引爆了整个NLP界。而BERT取得成功的一个关键因素是Transformer的强大作用。谷歌的Transformer模型最早是用于机器</description>
    </item>
    <item>
      <title>运行机制及和的异同比较</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6%E5%8F%8A%E5%92%8C%E7%9A%84%E5%BC%82%E5%90%8C%E6%AF%94%E8%BE%83/</link>
      <pubDate>Tue, 15 Mar 2022 09:02:31 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6%E5%8F%8A%E5%92%8C%E7%9A%84%E5%BC%82%E5%90%8C%E6%AF%94%E8%BE%83/</guid>
      <description>文章作者：张俊林 新浪微博 AI Lab 资深算法专家 内容来源：深度学习前沿笔记@知乎专栏 出品社区：DataFun 这两天，XLNet 貌似也引起了 NLP 圈的极大关注，从实验数据看，在某些场景下，确实 XLNet 相对 Bert 有很大幅度的提升。就像我们之前说的，感觉 Bert 打开两阶段模式的魔法盒开关后，在这条路上，会有越来越</description>
    </item>
    <item>
      <title>详解一文读懂模型</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%AF%A6%E8%A7%A3%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Tue, 15 Mar 2022 09:02:24 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%AF%A6%E8%A7%A3%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82%E6%A8%A1%E5%9E%8B/</guid>
      <description>作者： Microstrong 本文概览： 1. Autoregressive语言模型与Autoencoder语言模型 1.1 语言模型概念介绍 Autoregressive语言模型：指的是依据前面（或后面）出现的单词来预测当前时刻的单词，代表有 ELMo， GPT等。 Autoencoder语言模型：通过上下文信息来预测被</description>
    </item>
    <item>
      <title>谷歌自然语言处理模型基于</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%B0%B7%E6%AD%8C%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B%E5%9F%BA%E4%BA%8E/</link>
      <pubDate>Tue, 15 Mar 2022 08:58:49 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%B0%B7%E6%AD%8C%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B%E5%9F%BA%E4%BA%8E/</guid>
      <description>一、前言 最近谷歌搞了个大新闻，公司AI团队新发布的BERT模型，在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩：全部两个衡量指标上全面超越人类，并且还在11种不同NLP测试中创出最佳成绩，包括将GLUE基准推至80.4％（绝对改进7.6％），MultiNLI准确度达</description>
    </item>
    <item>
      <title>这些上下文相关的表示到底有多上下文化</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%BF%99%E4%BA%9B%E4%B8%8A%E4%B8%8B%E6%96%87%E7%9B%B8%E5%85%B3%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%88%B0%E5%BA%95%E6%9C%89%E5%A4%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8C%96/</link>
      <pubDate>Mon, 14 Mar 2022 18:02:18 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%BF%99%E4%BA%9B%E4%B8%8A%E4%B8%8B%E6%96%87%E7%9B%B8%E5%85%B3%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%88%B0%E5%BA%95%E6%9C%89%E5%A4%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8C%96/</guid>
      <description>作者：Kawin Ethayarajh 编译：ronghuaiyang 原文： 英文原文： https://kawine.github.io/blog/nlp/2020/02/03/contextual.html 导读： 具有上下文信息的词表示到底有多大程度的上下文化？这里给出了定量的分析。 将上下文信息放到词嵌入中 — 就像BERT，ELMo和GPT-2 — 已经证明了是NLP的一个分水岭的想法了。使用具有上下文信息的词表示来替换静态</description>
    </item>
    <item>
      <title>贝壳找房语言模型系列实践篇在房产领域的实践</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%B3%BB%E5%88%97%E5%AE%9E%E8%B7%B5%E7%AF%87%E5%9C%A8%E6%88%BF%E4%BA%A7%E9%A2%86%E5%9F%9F%E7%9A%84%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Mon, 14 Mar 2022 17:43:47 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%B3%BB%E5%88%97%E5%AE%9E%E8%B7%B5%E7%AF%87%E5%9C%A8%E6%88%BF%E4%BA%A7%E9%A2%86%E5%9F%9F%E7%9A%84%E5%AE%9E%E8%B7%B5/</guid>
      <description>贝壳找房【语言模型系列】原理篇一：从 one-hot 到 Word2vec 贝壳找房【语言模型系列】原理篇二：从 ELMo 到 ALBERT 随着预训练模型在各大榜单的不断屠榜，学术界和工业界对于预训练模型的研究也愈加狂热。预训练语言模型一般基于海量语料，消耗大量的硬件资源以及时间成本，利用无监督的方法学习一个语言模型，随之应用到各种任</description>
    </item>
    <item>
      <title>图解当前最强语言模型是如何攻克迁移学习的</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%9B%BE%E8%A7%A3%E5%BD%93%E5%89%8D%E6%9C%80%E5%BC%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%A6%82%E4%BD%95%E6%94%BB%E5%85%8B%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84/</link>
      <pubDate>Mon, 14 Mar 2022 16:40:30 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%9B%BE%E8%A7%A3%E5%BD%93%E5%89%8D%E6%9C%80%E5%BC%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%A6%82%E4%BD%95%E6%94%BB%E5%85%8B%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84/</guid>
      <description>作者：Jay Alammar 机器之心编译 参与：Panda 前段时间，谷歌发布了基于双向 Transformer 的大规模预训练语言模型 BERT，该预训练模型能高效抽取文本信息并应用于各种 NLP 任务，该研究凭借预训练模型刷新了 11 项 NLP 任务的当前最优性能记录。技术博主 Jay Alammar 近日发文通过图解方式生动地讲解了 BERT 的架构和方法基础。 2018 年是</description>
    </item>
  </channel>
</rss>
