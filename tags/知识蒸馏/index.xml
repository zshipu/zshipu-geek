<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>知识蒸馏 on 知识铺的博客</title>
    <link>https://geek.zshipu.com/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/</link>
    <description>Recent content in 知识蒸馏 on 知识铺的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 15 Mar 2022 09:21:52 +0800</lastBuildDate>
    <atom:link href="https://geek.zshipu.com/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>知识蒸馏如何用一个神经网络训练另一个神经网络</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E5%A6%82%E4%BD%95%E7%94%A8%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E5%8F%A6%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Tue, 15 Mar 2022 09:21:52 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E5%A6%82%E4%BD%95%E7%94%A8%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E5%8F%A6%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid>
      <description>作者：Tivadar Danka 编译：ronghuaiyang 导读: 知识蒸馏的简单介绍，让大家了解知识蒸馏背后的直觉。 如果你曾经用神经网络来解决一个复杂的问题，你就会知道它们的尺寸可能非常巨大，包含数百万个参数。例如著名的BERT模型约有1亿1千万参数。 为了说明这一点，参见下图中的NLP中</description>
    </item>
    <item>
      <title>知识蒸馏在推荐系统的应用</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E5%9C%A8%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%BA%94%E7%94%A8/</link>
      <pubDate>Tue, 15 Mar 2022 09:08:29 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E5%9C%A8%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%BA%94%E7%94%A8/</guid>
      <description>作者 | 张俊林@新浪微博 目录 知识蒸馏典型方法 知识蒸馏在推荐系统中的三个应用场景 知识蒸馏在三类推荐排序中的方法介绍 联合训练召回、粗排及精排模型的设想 随着深度学习的快速发展，优秀的模型层出不穷，比如图像领域的 ResNet、自然语言处理领域的 Bert，这些革命性的新技术使得应用效果快速提</description>
    </item>
    <item>
      <title>知乎搜索文本相关性与知识蒸馏</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%9F%A5%E4%B9%8E%E6%90%9C%E7%B4%A2%E6%96%87%E6%9C%AC%E7%9B%B8%E5%85%B3%E6%80%A7%E4%B8%8E%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/</link>
      <pubDate>Tue, 15 Mar 2022 09:05:21 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%9F%A5%E4%B9%8E%E6%90%9C%E7%B4%A2%E6%96%87%E6%9C%AC%E7%9B%B8%E5%85%B3%E6%80%A7%E4%B8%8E%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/</guid>
      <description>分享嘉宾：申站 知乎 算法工程师 编辑整理：许宴铭 出品平台：DataFunTalk 导读： 大家好，我是申站，知乎搜索团队的算法工程师。今天给大家分享下知乎搜索中文本相关性和知识蒸馏的工作实践，主要内容包括： 知乎搜索文本相关性的演进 BERT在知乎搜索的应用和问题 知识蒸馏及常见方案 知乎搜索在</description>
    </item>
  </channel>
</rss>
