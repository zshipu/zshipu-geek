<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>迁移学习 on 知识铺的博客</title>
    <link>https://geek.zshipu.com/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 迁移学习 on 知识铺的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 15 Mar 2022 09:05:19 +0800</lastBuildDate>
    <atom:link href="https://geek.zshipu.com/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>萨摩耶云深度迁移学习技术在金融风控中的应用</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%90%A8%E6%91%A9%E8%80%B6%E4%BA%91%E6%B7%B1%E5%BA%A6%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E5%9C%A8%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</link>
      <pubDate>Tue, 15 Mar 2022 09:05:19 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%90%A8%E6%91%A9%E8%80%B6%E4%BA%91%E6%B7%B1%E5%BA%A6%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E5%9C%A8%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</guid>
      <description>分享嘉宾：朱晓海 萨摩耶云 编辑整理：Hoh Xil 内容来源：作者授权发布 出品平台：DataFunTalk 导读： 迁移学习利用数据、模型之间的相似性，在不同领域之间进行知识迁移。深度学习技术与迁移学习思想的融合，又极大扩展了传统迁移学习技术的能力边界，给包括金融风控在内的各个场景带来了更多的</description>
    </item>
    <item>
      <title>图解当前最强语言模型是如何攻克迁移学习的</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%9B%BE%E8%A7%A3%E5%BD%93%E5%89%8D%E6%9C%80%E5%BC%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%A6%82%E4%BD%95%E6%94%BB%E5%85%8B%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84/</link>
      <pubDate>Mon, 14 Mar 2022 16:40:30 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%9B%BE%E8%A7%A3%E5%BD%93%E5%89%8D%E6%9C%80%E5%BC%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%A6%82%E4%BD%95%E6%94%BB%E5%85%8B%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84/</guid>
      <description>作者：Jay Alammar 机器之心编译 参与：Panda 前段时间，谷歌发布了基于双向 Transformer 的大规模预训练语言模型 BERT，该预训练模型能高效抽取文本信息并应用于各种 NLP 任务，该研究凭借预训练模型刷新了 11 项 NLP 任务的当前最优性能记录。技术博主 Jay Alammar 近日发文通过图解方式生动地讲解了 BERT 的架构和方法基础。 2018 年是</description>
    </item>
  </channel>
</rss>
