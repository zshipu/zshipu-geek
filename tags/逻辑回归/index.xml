<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>逻辑回归 on 知识铺的博客</title>
    <link>https://geek.zshipu.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</link>
    <description>Recent content in 逻辑回归 on 知识铺的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 15 Mar 2022 09:31:27 +0800</lastBuildDate>
    <atom:link href="https://geek.zshipu.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>黑盒模型实际上比逻辑回归更具可解释性</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E9%BB%91%E7%9B%92%E6%A8%A1%E5%9E%8B%E5%AE%9E%E9%99%85%E4%B8%8A%E6%AF%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%9B%B4%E5%85%B7%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/</link>
      <pubDate>Tue, 15 Mar 2022 09:31:27 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E9%BB%91%E7%9B%92%E6%A8%A1%E5%9E%8B%E5%AE%9E%E9%99%85%E4%B8%8A%E6%AF%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%9B%B4%E5%85%B7%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/</guid>
      <description>作者：Samuele Mazzanti 编译：ronghuaiyang 导读： 如何让复杂的模型具备可解释性，SHAP 值是一个很好的工具，但是 SHAP 值不是很好理解，如果能将 SHAP 值转化为对概率的影响，看起来就很舒服了。 在可解释性和高性能之间的永恒的争斗 从事数据科学工作的人更了解这一点：关于机器学习的一个老生常</description>
    </item>
    <item>
      <title>机器学习第二篇逻辑回归</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AF%87%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Mon, 14 Mar 2022 18:01:02 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AF%87%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</guid>
      <description>转载自 追梦程序员 公众号 前面介绍了机器学习中最简单的线性回归模型，机器学习第一篇——线性模型。今天，我们就来看看传说中的逻辑回归（logistic regression）。 大家首先想一想，如果我们想要用已求得的线性模型来完成对样本的二分类，一个最简单的做法如下。 这样虽然可以完成对数据</description>
    </item>
    <item>
      <title>逻辑回归模型融合原理详解与实战</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%88%98/</link>
      <pubDate>Mon, 14 Mar 2022 16:42:54 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%88%98/</guid>
      <description>来源： Datawhale干货 作者：吴忠强，东北大学，Datawhale成员 一、GBDT+LR简介 协同过滤和矩阵分解存在的劣势就是仅利用了用户与物品相互行为信息进行推荐， 忽视了用户自身特征， 物品自身特征以及上下文信息等，导致生成的结果往往会比较片面。而这次介绍的这个模型是2014年</description>
    </item>
    <item>
      <title>从损失函数的角度详解机器学习算法之逻辑回归</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E4%BB%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E8%A7%92%E5%BA%A6%E8%AF%A6%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Mon, 14 Mar 2022 16:42:39 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E4%BB%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E8%A7%92%E5%BA%A6%E8%AF%A6%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</guid>
      <description>源 | 机器学习算法全栈工程师 ID：Jeemy110 作者：章华燕 逻辑回归详解 分类是监督学习的一个核心问题，在监督学习中，当输出变量Y取有限个离散值时，预测问题便成为分类问题。这时，输入变量X可以是离散的，也可以是连续的。监督学习从数据中学习一个分类模型或分类决策函数，称为分类器(cl</description>
    </item>
  </channel>
</rss>
