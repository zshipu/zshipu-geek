<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformer on 知识铺的博客</title>
    <link>https://geek.zshipu.com/tags/Transformer/</link>
    <description>Recent content in Transformer on 知识铺的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 15 Mar 2022 09:59:07 +0800</lastBuildDate>
    <atom:link href="https://geek.zshipu.com/tags/Transformer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>算法工程师必知必会的经典模型系列一模型串讲</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E7%9A%84%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%E7%B3%BB%E5%88%97%E4%B8%80%E6%A8%A1%E5%9E%8B%E4%B8%B2%E8%AE%B2/</link>
      <pubDate>Tue, 15 Mar 2022 09:59:07 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E7%9A%84%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%E7%B3%BB%E5%88%97%E4%B8%80%E6%A8%A1%E5%9E%8B%E4%B8%B2%E8%AE%B2/</guid>
      <description>章立 美团点评算法工程师 未经许可 禁止转载 这是本系列的第一篇文章，期待大家关注我，跟进后续哦 ～ 注意力机制与 Transformer 模型分享 注意力机制 什么是注意力 注意力 的原型很容易理解，例如图片 👇 这张图里的要素很多，对于抱有不同目的或者习惯的人。会有注意到不同的内容。 如果我是对色彩比较敏感的人，那么可能 CSDN 的</description>
    </item>
    <item>
      <title>详解</title>
      <link>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Tue, 15 Mar 2022 09:08:38 +0800</pubDate>
      <guid>https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E8%AF%A6%E8%A7%A3/</guid>
      <description>前言 注意力（Attention）机制[2]由Bengio团队与2014年提出并在近年广泛的应用在深度学习中的各个领域，例如在计算机视觉方向用于捕捉图像上的感受野，或者NLP中用于定位关键token或者特征。谷歌团队近期提出的用于生成词向量的BERT[3]算法在NLP的11项任务中</description>
    </item>
  </channel>
</rss>
