<!doctype html>
<html lang="zh-CN">
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>干货推荐系统中的深度匹配模型 | 知识铺的博客</title>
    <meta property="og:title" content="干货推荐系统中的深度匹配模型 - 知识铺的博客">
    <meta property="og:type" content="article">
        
    <meta property="article:published_time" content='2022-03-15T11:20:32&#43;08:00'>
        
        
    <meta property="article:modified_time" content='2022-03-15T11:20:32&#43;08:00'>
        
    <meta name="Keywords" content="golang,go语言,go语言笔记,知识铺,java,android,博客,项目管理,python,软件架构,公众号,小程序">
    <meta name="description" content="干货推荐系统中的深度匹配模型">
        <meta name="author" content="知识铺">
        
    <meta property="og:url" content="https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%B9%B2%E8%B4%A7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B/">
    <link rel="shortcut icon" href='/favicon.ico'  type="image/x-icon">

    <link rel="stylesheet" href='/css/normalize.css'>
    <link rel="stylesheet" href='/css/style.css'>
    <script type="text/javascript" src="//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    
    <script data-ad-client="ca-pub-2874221941555456" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    
    
    
    
    
    
    
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-WLWJSST');</script>
    
</head>


<body>

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WLWJSST"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <a id="logo" href="https://geek.zshipu.com/">
                        知识铺的博客
                    </a>
                
                <p class="description">专注于Android、Java、Go语言(golang)、移动互联网、项目管理、软件架构</p>
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="current" href="https://geek.zshipu.com/">首页</a>
                    
                    <a  href="https://geek.zshipu.com/archives/" title="归档">归档</a>
                    
                    <a  href="https://geek.zshipu.com/about/" title="关于">关于</a>
                    
                </nav>
            </div>
        </div>
    </div>
</header>

    <div id="body">
        <div class="container">
            <div class="col-group">

                <div class="col-8" id="main">
                    
<div class="res-cons">
    <style type="text/css">
    .post-toc {
        position: fixed;
        width: 200px;
        margin-left: -210px;
        padding: 5px 10px;
        font-family: Athelas, STHeiti, Microsoft Yahei, serif;
        font-size: 12px;
        border: 1px solid rgba(0, 0, 0, .07);
        border-radius: 5px;
        background-color: rgba(255, 255, 255, 0.98);
        background-clip: padding-box;
        -webkit-box-shadow: 1px 1px 2px rgba(0, 0, 0, .125);
        box-shadow: 1px 1px 2px rgba(0, 0, 0, .125);
        word-wrap: break-word;
        white-space: nowrap;
        -webkit-box-sizing: border-box;
        box-sizing: border-box;
        z-index: 999;
        cursor: pointer;
        max-height: 70%;
        overflow-y: auto;
        overflow-x: hidden;
    }

    .post-toc .post-toc-title {
        width: 100%;
        margin: 0 auto;
        font-size: 20px;
        font-weight: 400;
        text-transform: uppercase;
        text-align: center;
    }

    .post-toc .post-toc-content {
        font-size: 15px;
    }

    .post-toc .post-toc-content>nav>ul {
        margin: 10px 0;
    }

    .post-toc .post-toc-content ul {
        padding-left: 20px;
        list-style: square;
        margin: 0.5em;
        line-height: 1.8em;
    }

    .post-toc .post-toc-content ul ul {
        padding-left: 15px;
        display: none;
    }

    @media print,
    screen and (max-width:1057px) {
        .post-toc {
            display: none;
        }
    }
</style>
<div class="post-toc" style="position: absolute; top: 188px;">
    <h2 class="post-toc-title">文章目录</h2>
    <div class="post-toc-content">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#推荐系统概述"><strong>推荐系统概述</strong></a>
      <ul>
        <li><a href="#11-推荐系统本质"><strong>1.1 推荐系统本质</strong></a></li>
        <li><a href="#12-推荐和搜索比较"><strong>1.2 推荐和搜索比较</strong></a></li>
      </ul>
    </li>
    <li><a href="#推荐系统的传统匹配模型"><strong>推荐系统的传统匹配模型</strong></a>
      <ul>
        <li><a href="#21-基于-collaborative-filtering-的方法"><strong>2.1 基于 Collaborative Filtering 的方法</strong></a></li>
        <li><a href="#22-generic-feature-based-的方法"><strong>2.2 Generic feature-based 的方法</strong></a></li>
        <li><a href="#23-传统模型总结"><strong>2.3 传统模型总结</strong></a></li>
      </ul>
    </li>
    <li><a href="#基于-representation-learning-的深度匹配模型"><strong>基于 representation learning 的深度匹配模型</strong></a>
      <ul>
        <li><a href="#31-基于-collaborative-filtering-的方法"><strong>3.1 基于 Collaborative Filtering 的方法</strong></a></li>
        <li><a href="#32-基于-collaborative-filtering-side-information-的方法"><strong>3.2 基于 Collaborative Filtering+ side information 的方法</strong></a></li>
        <li><a href="#33-基于-representation-的深度匹配方法总结"><strong>3.3 基于 representation 的深度匹配方法总结</strong></a></li>
      </ul>
    </li>
    <li><a href="#基于-match-function-learning-的深度匹配模型"><strong>基于 match function learning 的深度匹配模型</strong></a>
      <ul>
        <li><a href="#41-cf-based-的深度模型"><strong>4.1 CF-based 的深度模型</strong></a></li>
        <li><a href="#42-feature-based-的深度模型"><strong>4.2 feature-based 的深度模型</strong></a></li>
        <li><a href="#421-widedeep-模型"><strong>4.2.1 wide&amp;deep 模型</strong></a></li>
        <li><a href="#427-dcn-模型--deep-cross-network-"><strong>4.2.7 DCN 模型 ( Deep Cross Network )</strong></a></li>
        <li><a href="#43-feature-based-模型总结"><strong>4.3 Feature-based 模型总结</strong></a></li>
      </ul>
    </li>
    <li><a href="#总结"><strong>总结</strong></a></li>
  </ul>
</nav>
    </div>
</div>
<script type="text/javascript">
    $(document).ready(function () {
        var postToc = $(".post-toc");
        if (postToc.length) {
            var leftPos = $("#main").offset().left;
            if(leftPos<220){
                postToc.css({"width":leftPos-10,"margin-left":(0-leftPos)})
            }

            var t = postToc.offset().top - 20,
                a = {
                    start: {
                        position: "absolute",
                        top: t
                    },
                    process: {
                        position: "fixed",
                        top: 20
                    },
                };
            $(window).scroll(function () {
                var e = $(window).scrollTop();
                e < t ? postToc.css(a.start) : postToc.css(a.process)
            })
        }
    })
</script>
    <article class="post">
        <header>
            <h1 class="post-title">干货推荐系统中的深度匹配模型</h1>
        </header>
        <date class="post-meta meta-date">
            2022年3月15日
        </date>
        
        
        <div class="post-meta">
            <span id="busuanzi_container_page_pv">|<span id="busuanzi_value_page_pv"></span><span>
                    阅读</span></span>
        </div>
        
        
        <div class="post-content">
            <blockquote>
<p><strong>辛俊波</strong></p>
<p>腾讯 | 高级研究员</p>
</blockquote>
<h2 id="推荐系统概述"><strong>推荐系统概述</strong></h2>
<h3 id="11-推荐系统本质"><strong>1.1 推荐系统本质</strong></h3>
<p>推荐系统就是系统根据用户的属性 ( 如性别、年龄、学历、地域、职业 )，用户在系统里过去的行为 ( 例如浏览、点击、搜索、购买、收藏等 )，以及当前上下文环境 ( 如网络、手机设备、时间等 )，从而给用户推荐用户可能感兴趣的物品 ( 如电商的商品、feeds 推荐的新闻、应用商店推荐的 app 等 )，从这个过程来看，推荐系统就是一个给 user 匹配 ( match ) 感兴趣的 item 的过程。</p>
<h3 id="12-推荐和搜索比较"><strong>1.2 推荐和搜索比较</strong></h3>
<p>推荐和搜索有很多相同又有很多不同的地方，放到一起的原因是两者其实都是一个 match 的过程，图 1.1 展示的是一个搜索引擎的架构，需要 match 的是 query 和相关的 doc；图 1.2 展示的是一个推荐系统的架构，需要 match 的是 user ( 可能会带有主动意图的 query ) 和相关的 item。</p>
<p>图 1.1 搜索引擎架构：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-15e81d25edb8457ca24ecf159443cd41.webp" />   
    </p>
<p>图 1.2 推荐引擎架构：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-1bbeced31da5419a9a6d8b3fd1a0dcf1.png" />   
    </p>
<h4 id="121-搜索和推荐不同之处"><strong>1.2.1 搜索和推荐不同之处</strong></h4>
<p>❶ 意图不同</p>
<p>搜索是用户带着明确的目的，通过给系统输入 query 来主动触发的，搜索过程用户带着明确的搜索意图。而推荐是系统被动触发，用户是以一种闲逛的姿态过来的，系统是带着一种 &ldquo;猜&rdquo; 的状态给用户推送物品。</p>
<p>简单来说，搜索是一次主动 pull 的过程，用户用 query 告诉系统，我需要什么，你给我相关的结果就行；而推荐是一次 push 的过程，用户没有明显意图，系统给用户被动 push 认为用户可能会喜欢的东西吧。</p>
<p>❷ 时效不同</p>
<p>搜索需要尽快满足用户此次请求 query，如果搜索引擎无法满足用户当下的需求，例如给出的搜索结果和用户输入的 query 完全不相关，尽是瞎猜的结果，用户体验会变得很差。</p>
<p>推荐更希望能增加用户的时长和留存从而提升整体 LTV ( long time value，衡量用户对系统的长期价值 )，例如视频推荐系统希望用户能够持续的沉浸在观看系统推荐的视频流中；电商推荐系统希望用户能够多逛多点击推荐的商品从而提高 GMV。</p>
<p>❸ 相关性要求不同</p>
<p>搜索有严格的 query 限制，搜索结果需要保证相关性，搜索结果量化评估标准也相对容易。给定一个 query，系统给出不同结果，在上线前就可以通过相关性对结果进行判定相关性好坏。例如下图中搜索 query 为 &ldquo;pool schedule&rdquo;，搜索结果 &ldquo;swimming pool schedule&rdquo; 认为是相关的、而最后一个 case，用户搜索 &ldquo;why are windows so expensive&rdquo; 想问的是窗户为什么那么贵，而如果搜索引擎将这里的 windows 理解成微软的 windows 系统从而给出结果是苹果公司的 Mac，一字之差意思完全不同了，典型的 bad case。</p>
<p>图 1.3 部分 query 和 document 的相关性匹配：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-fddd21e630f140d084257756bf2e60d9.png" />   
    </p>
<p>而推荐没有明确的相关性要求。一个电商系统，用户过去买了足球鞋，下次过来推荐电子类产品也无法说明是 bad case，因为用户行为少，推完全不相关的物品是系统的一次探索过程。推荐很难在离线阶段从相关性角度结果评定是否好坏，只能从线上效果看用户是否买单做评估。</p>
<p>❹ 实体不同</p>
<p>搜索中的两大实体是 query 和 doc，本质上都是文本信息。这就是上文说到的为什么搜索可以通过 query 和 doc 的文本相关性判断是否相关。Query 和 doc 的匹配过程就是在语法层面理解 query 和 doc 之间 gap 的过程。</p>
<p>推荐中的两大实体是 user 和 item，两者的表征体系可能完全没有重叠。例如电影推荐系统里，用户的特征描述是：用户 id，用户评分历史、用户性别、年龄；而电影的特征描述是：电影 id，电影描述，电影分类，电影票房等。这就决定了推荐中，user 和 item 的匹配是无法从表面的特征解决两者 gap 的。</p>
<p>图 1.4 推荐系统的两大实体：user 和 item:</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-a1a5d6b7aa824a68b1934548e6968745.jpeg" />   
    </p>
<p>❺ 个性化要求不同</p>
<p>虽然现在但凡是一个推荐系统都在各种标榜如何做好个性化，&ldquo;千人千面&rdquo;，但搜索和推荐天然对个性化需求不同。搜索有用户的主动 query，本质上这个 query 已经在告诉系统这个 &ldquo;用户&rdquo; 是谁了，query 本身代表的就是一类用户，例如搜索引擎里搜索 &ldquo;深度学习综述&rdquo; 的本质上就代表了机器学习相关从业者或者对其感兴趣的这类人。在一些垂直行业，有时候 query 本身就够了，甚至不需要其他用户属性画像。例如在 app 推荐系统里，不同的用户搜索 &ldquo;京东&rdquo;，并不会因为用户过去行为、本身画像属性不同而有所不同。</p>
<p>图 1.5 应用宝搜索京东，不太需要很强的个性化结果：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-6d95b44213bf477ea203179c9a163567.webp" />   
    </p>
<p>而推荐没有用户主动的 query 输入，如果没有用户画像属性和过去行为的刻画，系统基本上就等于瞎猜。</p>
<h4 id="122-搜索和推荐相同之处"><strong>1.2.2 搜索和推荐相同之处</strong></h4>
<p>❶ 本质是都是 match 过程</p>
<p>如果把 user 比作 query，把 item 比作 doc，那么推荐和搜索在这个层面又是相同的，都是针对一个 query ( 一个 user )，从海量的候选物品库中，根据 query 和 doc 的相关性 ( user 过去的历史、画像等和 item 的匹配程度 )，去推荐匹配的 doc ( item )。</p>
<p>❷ 目标相同</p>
<p>搜索和推荐的目标都是针对一次 context ( 或者有明确意图，或者没有 )，从候选池选出尽可能满足需求的物品。两者区别只是挑选过程使用的信息特征不同。</p>
<p>❸ 语义鸿沟 ( semantic gap ) 都是两者最大的挑战</p>
<p>在搜索里表现是 query 和 doc 的语义理解，推荐里则是 user 和 item 的理解。例如，搜索里多个不同的 query 可能表示同一个意图；而推荐里用来表示 user 和 item 的特征体系可能完全不是一个层面的意思。</p>
<h2 id="推荐系统的传统匹配模型"><strong>推荐系统的传统匹配模型</strong></h2>
<h3 id="21-基于-collaborative-filtering-的方法"><strong>2.1 基于 Collaborative Filtering 的方法</strong></h3>
<h4 id="211-cf-模型"><strong>2.1.1 CF 模型</strong></h4>
<p>说到推荐系统里最经典的模型，莫过于大名鼎鼎的协同过滤了。协同过滤基于一个最基本的假设：一个用户的行为，可以由和他行为相似的用户进行预测。</p>
<p>协同过滤的基本思想是基于 &lt;user, item&gt; 的所有交互行为，利用集体智慧进行推荐。CF 按照类型可以分为 3 种，user-based CF、item-based CF 和 model-based CF。</p>
<p>❶ User-base CF：通过对用户喜欢的 item 进行分析，如果用户 a 和用户 b 喜欢过的 item 差不多，那么用户 a 和 b 是相似的。类似朋友推荐一样，可以将 b 喜欢过但是 a 没有看过的 item 推荐给 a。</p>
<p>❷ Item-base CF: item A 和 item B 如果被差不多的人喜欢，认为 item A 和 item B 是相似的。用户如果喜欢 item A，那么给用户推荐 item B 大概率也是喜欢的。比如用户浏览过这篇介绍推荐系统的文章，也很有可能会喜欢和推荐系统类似的其他机器学习相关文章。</p>
<p>❸ Model-base CF: 也叫基于学习的方法，通过定义一个参数模型来描述用户和物品、用户和用户、物品和物品之间的关系，然后通过已有的用户-物品评分矩阵来优化求解得到参数。例如矩阵分解、隐语义模型 LFM 等。</p>
<p>CF 协同过滤的思路要解决的问题用数据形式表达就是：矩阵的未知部分如何填充问题 ( Matrix Completion )。如图 2.1 所示，已知的值是用户已经交互过的 item，如何基于这些已知值填充矩阵剩下的未知值，也就是去预测用户没有交互过的 item 是矩阵填充要解决的问题。</p>
<p>图 2.1 用户对左图评分过的电影，可以用右图矩阵填充表达：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-7e60d6e0ab014a839d83c0a8ed8b824e.webp" />   
    </p>
<p>矩阵填充可以用经典的 SVD ( Singular Value Decomposition ) 解决，如图 2.1 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-fd9d7cc1f1824d7d951bddf5fcdb94f7.webp" />   
    </p>
<p>图 2.2 SVD 矩阵分解</p>
<p>其中左侧 M=m*n 表示用户评分矩阵，m 矩阵的行表示用户数，n 矩阵的列表示 item 数，在大多数推荐系统中 m 和 n 规模都比较大，因此希望通过将 M 分解成右侧低秩的形式。一般来说 SVD 求解可以分为三步：</p>
<p>❶ 对 M 矩阵的 missing data 填充为 0</p>
<p>❷ 求解 SVD 问题，得到 U 矩阵和 V 矩阵</p>
<p>❸ 利用 U 和 V 矩阵的低秩 k 维矩阵来估计</p>
<p>对于第二步种的 SVD 求解问题，等价于以下的最优化问题：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-047aadad5c1b46379095e7bee624eec1.webp" />   
    </p>
<p>其中 y_{ij}为用户 i 对物品 j 的真实评分，也就是 label，U 和 V 为模型预估值，求解矩阵 U 和 V 的过程就是最小化用户真实评分矩阵和预测矩阵误差的过程。</p>
<p>这种 SVD 求解方法存在以下问题：</p>
<p>❶ Missing data ( 在数据集占比超过 99% ) 和 observe data 权重一样。</p>
<p>❷ 最小化过程没有正则化 ( 只有最小方差 )，容易产生过拟合。</p>
<p>因此，一般来说针对原始的 SVD 方法会有很多改进方法。</p>
<h4 id="212-mf-模型--矩阵分解-"><strong>2.1.2 MF 模型 ( 矩阵分解 )</strong></h4>
<p>为解决上述过拟合情况，矩阵分解模型 ( matrix factorization ) 提出的模型如下：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-117e452f2c484168afefa283239d50c8.png" />   
    </p>
<p>MF 模型的核心思想可以分成两步：</p>
<p>❶ 将用户 u 对物品 i 的打分分解成用户的隐向量 v_u，以及物品的隐向量 v_i；</p>
<p>❷ 用户 u 和物品 i 的向量点积 ( inner product ) 得到的 value，可以用来代表用户 u 对物品 i 的喜好程度，分数越高代表该 item 推荐给用户的概率就越大。</p>
<p>同时，MF 模型引入了 L2 正则来解决过拟合问题。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-8346305ea84a4790853d6dd9e110adc5.png" />   
    </p>
<p>当然，这里除了用 L2 正则，其他正则手段例如 L1 正则，cross-entropy 正则也都是可以的。</p>
<h4 id="213-fism-模型"><strong>2.1.3 FISM 模型</strong></h4>
<p>上述提到的两种模型 CF 方法和 MF 方法都只是简单利用了 user-item 的交互信息，对于用户本身的表达是 userid 也就是用户本身。2014 年 KDD 上提出了一种更加能够表达用户信息的方法，Factored Item Similarity Model，简称 FISM，顾名思义，就是将用户喜欢过的 item 作为用户的表达来刻画用户，用数据公式表示如下：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-74d5cd07f9c6418086d56526f80287b2.png" />   
     注意到用户表达不再是独立的隐向量，而是用用户喜欢过的所有 item 的累加求和得到作为 user 的表达；而 item 本身的隐向量 v_i是另一套表示，两者最终同样用向量内积表示。</p>
<h4 id="214-svd-模型"><strong>2.1.4 SVD++ 模型</strong></h4>
<p>MF 模型可以看成是 user-based 的 CF 模型，直接将用户 id 映射成隐向量，而 FISM 模型可以看成是 item-based 的 CF 模型，将用户交户过的 item 的集合映射成隐向量。一个是 userid 本身的信息，一个是 user 过去交互过的 item 的信息，如何结合 user-base 和 item-base 这两者本身的优势呢？</p>
<p>SVD++ 方法正是这两者的结合，数学表达如下：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-40b4b590d9fd4e9b919055ca9b74a67c.png" />   
    </p>
<p>其中，每个用户表达分成两个部分，左边 v_u表示用户 id 映射的隐向量 ( user-based CF 思想 )，右边是用户交互过的 item 集合的求和 ( item-based CF 思想 )。User 和 item 的相似度还是用向量点击来表达。</p>
<p>这种融合方法可以看成早期的模型融合方法，在连续 3 年的 Netflix 百万美金推荐比赛中可是表现最好的模型。</p>
<h3 id="22-generic-feature-based-的方法"><strong>2.2 Generic feature-based 的方法</strong></h3>
<p>上述的方法中，无论是 CF，MF，SVD，SVD++，还是 FISM，都只是利用了 user 和 item 的交互信息 ( ratin g data )，而对于大量的 side information 信息没有利用到。例如 user 本身的信息，如年龄，性别、职业；item 本身的 side information，如分类，描述，图文信息；以及 context 上下文信息，如位置，时间，天气等。因此，传统模型要讲的第二部分，是如何利用这些特征，去构造 feature-based 的 model。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-ccc928d563b147498afa4106e845edc3.png" />   
    </p>
<p>图 2.3 特征体系三模块：用户信息、物品信息、交互信息</p>
<h4 id="221-fm-模型"><strong>2.2.1 FM 模型</strong></h4>
<p>首先要介绍的是大名鼎鼎的 FM 模型。FM 模型可以看成由两部分组成，如图 2.4 所示，蓝色的 LR 线性模型，以及红色部分的二阶特征组合。对于每个输入特征，模型都需要学习一个低维的隐向量表达 v，也就是在各种 NN 网络里所谓的 embedding 表示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-b8f727e7e33c42328a61b5964ebe4a3a.webp" />   
    </p>
<p>图 2.4 FM 模型的稀疏 one-hot 特征输入</p>
<p>FM 模型的数学表达如图 2.5 所示：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-c41f1dd96d96449e8f6b65b81765868f.png" />   
    </p>
<p>图 2.5 FM 模型的数学表达分解</p>
<p>注意红色部分表示的是二阶特征的两两组合 ( 特征自己和自己不做交叉 )，向量之间的交叉还是用向量内积表示。FM 模型是 feature-based 模型的一个范式表达，接下来介绍的几个模型都可以看成是 FM 模型的特殊范例。</p>
<h4 id="222-fm-模型和-mf-关系"><strong>2.2.2 FM 模型和 MF 关系</strong></h4>
<p>假如只使用 userid 和 itemid，我们可以发现其实 FM 退化成加了 bias 的 MF 模型，如图 2.6 所示：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-a98ab645aecd4e4197bc3c7d6363b6ae.webp" />   
    </p>
<p>图 2.6 FM 模型可以退化成带 bias 的 MF 模型</p>
<p>数学表达式如下：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-677af15dbab24c44a2f5bbfb47f2cb7a.png" />   
    </p>
<h4 id="223-fm-模型和-fism-关系"><strong>2.2.3 FM 模型和 FISM 关系</strong></h4>
<p>如果输入包含两个变量，① 用户交互过的 item 集合；② itemid 本身，那么，此时的 FM 又将退化成带 bias 的 FISM 模型，如图 2.7 所示，蓝色方框表示的是用户历史交互过的 item ( rated movies )，右边橙色方框表示的是 itemid 本身的 one-hot 特征。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-be99f8a43a694943b10abf410e8a704d.webp" />   
    </p>
<p>图 2.7 FM 模型可以退化成带 bias 的 FISM 模型</p>
<p>此时的 FM 模型数学表达如下：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-6c4dffe86d7540ff82eb7b7732f52049.webp" />   
    </p>
<p>同样道理，如果再加上 userid 的隐向量表达，那么 FM 模型将退化成 SVD++ 模型。可见 MF，FISM，SVD++ 其实都是 FM 的特例。</p>
<h3 id="23-传统模型总结"><strong>2.3 传统模型总结</strong></h3>
<p>上面介绍的模型都是通过打分预测来解决推荐系统的排序问题，这在很多时候一般都不是最优的，原因有如下几个方面：</p>
<p>❶ 预测打分用的 RMSE 指标和实际的推荐系统排序指标的 gap：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-d2a5ba76b24f4c3a86c1ae2921abadef.png" />   
    </p>
<p>预测打分用的 RMSE 拟合的是最小方差 ( 带正则 )，而实际面临的是个排序问题。</p>
<p>❷ 观察数据天然存在 bias</p>
<p>用户一般倾向于给自己喜欢的 item 打分，而用户没有打分过的 item 未必就真的是不喜欢。针对推荐系统的排序问题，一般可以用 pairwise 的 ranking 来替代 RMSE。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-12bd6ac6551346238c3e6d0eb4d1cad8.png" />   
    </p>
<p>如上述公式所示，不直接拟合用户对 item 的单个打分，而是以 pair 的形式进行拟合；一般来说，用户打分高的 item &gt; 用户打分低的 item；用户用过交互的 item &gt; 用户未交互过的 item ( 不一定真的不喜欢 )。</p>
<h2 id="基于-representation-learning-的深度匹配模型"><strong>基于 representation learning 的深度匹配模型</strong></h2>
<p>终于要讲到激动人心的深度学习部分了。深度学习匹配模型从大致方向上可以分为两大类，分别是基于 representation learning 的模型以及 match function learning 的模型。</p>
<p>本章主要讲述第一种方法，representation learning，也就是基于表示学习的方法。这种方法会分别学习用户的 representation 以及 item 的 representation，也就是 user 和 item 各自的 embedding 向量 ( 或者也叫做隐向量 )，然后通过定义 matching score 的函数，一般是简单的向量点击、或者 cosine 距离来得到两者的匹配分数。整个 representation learning 的框架如图 3.1 所示，是个典型的 user 和 item 的双塔结构：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-20b22ffcbef547118b131612c7e7aaa8.webp" />   
    </p>
<p>图 3.1 基于 representation learning 的匹配模型</p>
<p>基于 representation learning 的深度学习方法，又可以分为两大类，基于 CF 以及 CF + side info 的方法。下面的介绍将分别从 input 、representation function 和 matching function 三个角度分别看不同的模型有什么不同。</p>
<h3 id="31-基于-collaborative-filtering-的方法"><strong>3.1 基于 Collaborative Filtering 的方法</strong></h3>
<h4 id="311-cf-模型--collaborative-filtering-">3.1.1 CF 模型 ( collaborative filtering )</h4>
<p>重新回顾下传统方法里的协同过滤方法，如果从表示学习的角度来看，就是个经典的 representation learning 的模型，分别学习 user 和 item 的隐向量。</p>
<p>❶ input layer：只有两个，分别是 userid ( one-hot )，itemid ( one-hot )</p>
<p>❷ representation function：线性 embedding layer</p>
<p>❸ matching function：向量内积 ( inner product )</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-cd60cb25fece400f8912df961aa1c066.webp" />   
    </p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-92124a793f604064adc7a37d747831e4.webp" />   
    </p>
<p>图 3.2 CF 是 representation learning 最基础的模型</p>
<h4 id="312-模型--deep-matrix-factorization-"><strong>3.1.2 模型 ( Deep Matrix Factorization )</strong></h4>
<p>DMF 模型也就是深度矩阵分解模型，在传统的 MF 中增加了 MLP 网络，整个网络框架如图 3.3 所示。</p>
<p>❶ input layer</p>
<p>由两部分组组成，其中 user 由 user 交互过的 item 集合来表示，是个 multi-hot 的打分表示，如 [0 0 4 0 0 … 1 5 …]，在矩阵中用行表示；item 也由交互过的 user 集合来表示，也是个 multi-hot 的表示，如 [5 0 0 3 … 1 3]，在矩阵中用列表示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-d6e3210da26546e8ae3735fd83c44801.webp" />   
    </p>
<p>图 3.3 DMF 深度矩阵分解模型框架</p>
<p>可以发现这里的输入都是 one-hot 的，一般来说 M 用户数比较大，N 作为 item 数量假设是百万级别的。</p>
<p>❷ representation function</p>
<p>Multi-Layer-Perceptron，也就是经典的全连接网络。</p>
<p>❸ matching function</p>
<p>用 cosine 点击表示两个向量的匹配分数。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-482f039991834dd6b7cc0a9b0a04756c.webp" />   
    </p>
<p>对比普通的 CF 模型，最大的特点是在 representation function 中，增加了非线性的 MLP，但是由于输入是 one-hot 的，假设用户规模是 100 万，MLP 的第一层隐层是 100，整个网络光 user 侧的第一层参数将达到 1 亿，参数空间将变得非常大。</p>
<h4 id="313-autorec-模型"><strong>3.1.3 AutoRec 模型</strong></h4>
<p>借鉴 auto-encoder 的思路，AutoRec 模型对输入做重建，来建立 user 和 item 的 representation，和 CF 一样，也可以分为 user-based 和 item-based 的模型。对于 item-based AutoRec，input 为 R 里的每列，即每个 item 用各个 user 对它的打分作为其向量描述；对于 user-based AutoRec 则是用 R 里的每行来表示，即每个 user 用他打分过的 item 的向量来表达。</p>
<p>用r_u表示用户向量，r_i表示 item 向量，通过 autoencoder 将r_u或者r_i投射到低维向量空间 ( encode 过程 )，然后再将其投射到正常空间 ( decode 过程 )，利用 autoencoder 中目标值和输入值相近的特性，从而重建 ( reconstruct ) 出用户对于未交互过的 item 的打分。</p>
<p>❶ input layer</p>
<p>和 DMF 一样，user 用 user 作用过的 item 集合表示，item 则用 itemid 本身表示，图中在原 slides 是说 user-autoencoder，但个人在看原始 autoRec 论文时，这块应该有误，应该是 item-based 的，因为 m 表示的是用户数，n 表示 item 数，下方的输入表示所有 user(1,2,3,…m) 对 item i 的交互输入。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-f14e2c7955974a26bdf5cf783a738e89.webp" />   
    </p>
<p>图 3.4 item-based 的 autoRec 模型</p>
<p>❷ representation function</p>
<p>通过 auto-encoder 的结构表示，其中，h(r; theta) 表示的是输入层到隐层的重建；由于输入的是用户交互过的 item(multi-hot)，所以在隐层中的蓝色节点表示的就是 user representation；而输出的节点表示的是 item 的 representation，这样就可以得到 user 和 item 各自 representation，如下面公式所示：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-83461cb60c2449d4aea89c14155aa7fd.webp" />   
    </p>
<p>损失函数为最小化预测的平方差以及 W 和 V 矩阵的 L2 正则：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-f51217fb71fa4ae38e8a23d1ee436fad.png" />   
    </p>
<p>❸ matching function</p>
<p>有了 user 和 item 的 representation，就可以用向量点击得到两者的匹配分数。</p>
<h4 id="314-模型--collaborative-denoising-auto-encoders-"><strong>3.1.4 模型 ( Collaborative Denoising Auto-Encoders )</strong></h4>
<p>CDAE 模型类似 SVD++ 的思想，除了 userid 本身表达用户，也将用户交互过的 item 作为 user 的表达。</p>
<p>❶ input layer</p>
<p>用户 id，用户历史交互过的 item；以及 itemid。可以发现对比上述基础的 autoRec，用户侧输入同时使用了用户历史交互过的 item 以及 userid 本身这个 bias，思想很类似 SVD++。如图 3 所示的 input layer 节点，绿色节点表示每个用户交互过的 item，最下面的红色节点 user node 表示用户本身的偏好，可以认为是 userid 的表达。</p>
<p>❷ representation function</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-0dafc9cce42a4751bd97e270fe643388.webp" />   
    </p>
<p>图 3.5 CDAE 模型结构</p>
<p>其中，中间蓝色的隐层节点作为用户表示，其中V_u为 input layer 中的 user node 的 representation，针对所有用户 id 会学习一个和 item 无关的V_u向量表达，可以认为是用户本身的 bias，例如有些用户打分本身比较严格，再好的 item 打分也不会太高；有些用户打分很宽松，只要 item 别太差都会给高分，加上 V_u可以更好的刻画用户之间天然的 bias。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-2055b9e557b84b1189a2ca233aa9ee04.png" />   
    </p>
<p>而对于输出层的节点，可以认为是用户 u 对物品 i 的打分预测：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-faa87108084045fb8ffeba131b2a11fd.webp" />   
    </p>
<p>❸ matching function</p>
<p>使用向量点积作为匹配分数：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-9a45c1d54dd8475385c52c4f0eb288c4.png" />   
    </p>
<h4 id="315-基于-cf-方法的深度模型总结"><strong>3.1.5 基于 CF 方法的深度模型总结</strong></h4>
<p>总结下以上基于 CF 的方法，有以下几个特点：</p>
<p>❶ User 或者 item 要么由本身 id 表达，要么由其历史交互过的行为来表达；</p>
<p>❷ 用历史交互过的行为来作为 user 或者 item 的表达，比用 id 本身表达效果更好，但模型也变得更复杂；</p>
<p>❸ Auto-encoder 本质上等同于 MLP+MF，MLP 用全连接网络做 user 和 item 的 representation 表达；</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-4a7437f2c3b54979b9c342f1fc527db2.webp" />   
    </p>
<p>❹ 所有训练数据仅用到 user-item 的交互信息，完全没有引入 user 和 item 的 side info 信息。</p>
<h3 id="32-基于-collaborative-filtering-side-information-的方法"><strong>3.2 基于 Collaborative Filtering+ side information 的方法</strong></h3>
<p>基于 CF 的方法没有引入 side information，因此，对于 representation learning 的第二种方法，是基于 CF + side info，也就是在 CF 的方法上额外引入了 side info。</p>
<h4 id="321-dcf-模型--deep-collaborative-filtering-"><strong>3.2.1 DCF 模型 ( Deep Collaborative Filtering )</strong></h4>
<p>❶ input layer</p>
<p>除了用户和物品的交互矩阵，还有用户特征 X 和物品特征 Y。</p>
<p>❷ representation function</p>
<p>和传统的 CF 表示学习不同，这里引入了用户侧特征 X 例如年龄、性别等；物品侧特征 Y 例如文本、标题、类目等；user 和 item 侧的特征各自通过一个 auto-encoder 来学习，而交互信息 R 矩阵依然做矩阵分解 U,V。整个模型框架如图 3.6 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-a544c5d61881432eb34e85c90af4eaf3.webp" />   
    </p>
<p>图 3.6 DCF 模型框架</p>
<p>损失函数优化，需要最小化用户侧特征的 reconstruction 和 item 侧的 encoder 部分，以及交互矩阵和预测矩阵的平方差，还有加上 L2 正则。如图 3.7 第一个公式。</p>
<p>其中 W_1，表示的用户侧特征 X 在 auto-encoder 过程中的 encode 部分，也就是输入到隐层的重建，P_1表示的是用户特征到交互矩阵 R 的映射；而 W_2表示物品侧特征 Y 在 auto-encoder 过程中的 encode 部分。P_2表示的是物品特征到交互矩阵 R 的映射。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-95150c1109df41d58698da0b5f7cceb9.jpeg" />   
    </p>
<p>图 3.7 CDAE 模型的损失函数分解</p>
<p>图 3.7 下面两组公式中，可以看出用户侧和物品侧特征都由两项 error 组成，第一项衡量的是 input 和 corrupted input 构建的预估误差，需要保证 W_1和 W_2对于 corrupted 后的 input x 和 y 不能拟合太差；第二项表达的是映射后的隐层特征空间 W_1X 和投射到 U 矩阵的误差不能太大。</p>
<p>简单理解，整个模型的学习，既需要保证用户特征 X 和物品特征 Y 本身 encode 尽可能准确 ( auto-encoder 的 reconstruction 误差 )，又需要保证用户对物品的预估和实际观测的尽可能接近 ( 矩阵分解误差 )，同时正则化也约束了模型的复杂度不能太高。</p>
<h4 id="322-duif-模型--deep-user-and-image-feature-learning-"><strong>3.2.2 DUIF 模型 ( Deep User and Image Feature Learning )</strong></h4>
<p>❶ input layer</p>
<p>除了用户和物品的交互矩阵，还有用户特征 X 和物品特征 Y。</p>
<p>❷ representation function</p>
<p>整个 match score 可以用下图表示：f_i表示原始图片特征，通过 CNN 网络提取的图片特征作为 item 的表达，然后用一个线性映射可以得到 item 的 embedding 表达。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-00d55a8ec3bb4384a8989e2db763bdd4.webp" />   
    </p>
<p>❸ match function</p>
<p>通过模型学到的 p_u作为用户的 representation，以及通过 CNN 提取的图片特征作为 item 的 representation，两者通过向量点积得到两者的匹配分数。</p>
<h4 id="323-acf-模型--attentive-collaborative-filtering-"><strong>3.2.3 ACF 模型 ( Attentive Collaborative Filtering )</strong></h4>
<p>Sigir2017 提出的 Attention CF 方法，在传统的 CF 里引入了 attention 机制。这里的 attention 有两层意思，第一层 attention，认为用户历史交互过的 item 的权重是不一样的；另一个 attention 意思是，用户同一个 item 里到的视觉特征的权重也是不一样的，如图 3.8 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-f3d1f4e248894a4fa1cbfc036088aef8.webp" />   
    </p>
<p>图 3.8 ACF 模型结构</p>
<p>❶ input layer</p>
<p>① 用户侧：userid；用户历史交互过的 item。</p>
<p>② Item 侧：itemid；item 相关的视觉相关特征。</p>
<p>❷ representation function</p>
<p>可以分为两个 attention，一个是 component 层级的 attention，主要是提取视觉特征；第二层是 item 层级的 attention，主要提取用户对物品的喜好程度权重。</p>
<p>① component-attention</p>
<p>在该 paper 里的推荐系统针对的是 multi-media 的，有很多图文和视频的特征信息提取，所以引入的第一层 attention 指的是 component attention，认为对于不同的 components 对 item representation 的贡献程度是不同的，如图 3.9 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-e3b821b44cc6488486f5bf1b1b5e5dc4.webp" />   
    </p>
<p>图 3.9 component attention 框架</p>
<p>对第 l 个 item，输入为不同 region 本身的特征x_{I1}, x_{I2}, x_{Im}，表示的是 m 个不同的 item feature, 以及用户输入u_i，最终 item 的表达为不同的 region 的加权 embedding。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-eab38d02ff0c4d87b863423922379259.webp" />   
    </p>
<p>其中第一个公式表示用户 i 对物品 l 第 m 个 component ( 例如图片特征中的局部区域特征，或者视频中不同帧的特征 ) 的权重；第二个公式 softmax 对 attention 权重归一化。</p>
<p>② item attention</p>
<p>第二层 attention，认为用户作用过的 item 历史中，权重应该是不同的。这里文章使用了 SVD++ 的方式，用户本身的表达引入了 a(i, l)，代表的是用户 i 对其历史交互过的物品 l 的权重。</p>
<p>用户 i 对第 l 个 item 的权重表达可以用如下的数据表示：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-cb7b4c5063e4426f95347953115128db.webp" />   
    </p>
<p>其中 u_i是用户本身的 latent vector，v_I是物品 l 的 latent vector，p_I是物品 l 的辅助 latent vector；x_I是表示前面提到的从图文信息提取的特征 latent vector。用户最终的表达是自身 u_i的 latent vector，以及历史行为的 attention 加权的 representation 表示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-5e0c4aa3df304d448938cbc7c7f2184f.webp" />   
    </p>
<p>模型使用的是 pairwise loss 进行优化：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-b2f9a07a6adc4994afec28d5794f607e.webp" />   
    </p>
<p>❸ representation function</p>
<p>使用 user 和 item 的向量点击作为匹配分数。</p>
<h4 id="324-ckb-模型--collaborative-knowledge-base-embedding-"><strong>3.2.4 CKB 模型 ( Collaborative Knowledge Base Embedding )</strong></h4>
<p>CKB 模型是在 2016 年 KDD 提出的，利用知识图谱做 representation learning，模型框架如图 3.10 所示。整个 CKB 模型框架其实思想比较简单，分别在结构化信息、文本信息和视觉信息中提取 item 侧特征作为 item 的 representation。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-06f386dc59d14e548db8e01c861c0ede.jpeg" />   
    </p>
<p>图 3.10 CKB 模型框架</p>
<p>❶ input layer</p>
<p>① user 侧：userid</p>
<p>② item 侧：itemid；基于知识图谱的 item 特征 ( structural，textual，visual )</p>
<p>❷ representation function</p>
<p>主要是从知识图谱的角度，从结构化信息，文本信息以及图文信息分别提取 item 侧的表达，最终作为 item 的 embedding。</p>
<p>① 结构化特征 struct embedding: transR，transE</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-83c3b8b0e54244139f93c8d820f3e941.webp" />   
    </p>
<p>图 3.11 struct embedding 框架</p>
<p>② 文本特征 Textual embedding: stacked denoising auto-encoders ( S-DAE )</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-6f575d3ba06d41a7b9aae28f07d8616a.webp" />   
    </p>
<p>图 3.12 textual embedding 框架</p>
<p>③ 视觉特征 Visual embedding: stacked convolutional auto-encoders ( SCAE )</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-b4026febdd724b5ea7d1c96dd0d714e6.webp" />   
    </p>
<p>图 3.13 visual embedding 框架</p>
<p>❸ matching function</p>
<p>得到用户向量和 item 向量后，用向量点击表示 user 和 item 的匹配分数；损失函数则用如下的 pair-wise loss 表示：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-a11a3a005fb24c53a536fe9dba0d8a9a.png" />   
    </p>
<h3 id="33-基于-representation-的深度匹配方法总结"><strong>3.3 基于 representation 的深度匹配方法总结</strong></h3>
<p>总结上述基于 CF 的方法，可以用如下的范式作为表达：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-27a580f1d12e4cf29597f72c1d16e14f.jpeg" />   
    </p>
<p>图 3.14 基于 CF 的深度匹配模型范式</p>
<p>❶ representation learning：目的是学习到 user 和 item 各自的 representation ( 也叫 latent vector，或者 embedding )。</p>
<p>❷ 特征表达：user 侧特征除了用户 id 本身 userid，可以加上其他 side info；item 侧特征除了物品 id 本身 itemid，还有其他文本特征、图文特征、视频帧特征等信息。</p>
<p>❸ 模型表达：除了传统的 DNN，其他结构如 Auto-Encoder ( AE )，Denoise-Auto-Encoder ( DAE )，CNN，RNN 等。</p>
<p>基于 representation learning 的深度匹配模型不是一个 end-2-end 模型，通过 user 和 item 各自的 representation 作为中间产物，解释性较好，而且可以用在出了排序阶段以外的其他环节，例如求物品最相似的 item 集合，召回环节等。</p>
<h2 id="基于-match-function-learning-的深度匹配模型"><strong>基于 match function learning 的深度匹配模型</strong></h2>
<p>对比 representation learning 的方法，基于 match function learning 最大的特点是，不直接学习 user 和 item 的 embedding，而是通过已有的各种输入，通过一个 neural network 框架，来直接拟合 user 和 item 的匹配分数。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-f6e945a965b8425d8c6629bd01d2635d.webp" />   
    </p>
<p>图 4.1 基于 match function learning 的深度匹配模型框架</p>
<p>简单来说，第一种方法 representation learning 不是一种 end-2-end 的方法，通过学习 user 和 item 的 embedding 作为中间产物，然后可以方便的计算两者的匹配分数；而第二种方法 matching function learning 是一种 end2end 的方法，直接拟合得到最终的匹配分数。本章主要介绍基于 match function learning 的深度学习匹配方法。</p>
<h3 id="41-cf-based-的深度模型"><strong>4.1 CF-based 的深度模型</strong></h3>
<p>前面传统匹配模型以及基于表示学习的模型，其 base 模型都离不开协同过滤，也可以称为基于矩阵分解的模型。基于 match function learning 的模型也不例外。</p>
<h4 id="411-基于-ncf-框架的方法"><strong>4.1.1 基于 NCF 框架的方法</strong></h4>
<p>基于神经网络的学习方法 ( NCF ) 为何向南博士在 2017 年提出，对比传统的 CF 网络，在得到 user vector 和 item vector 后，连接了 MLP 网络后，最终拟合输出，得到一个 end-2-end 的 model。这套框架好处就是足够灵活，user 和 item 侧的双塔设计可以加入任意 side info 的特征，而 MLP 网络也可以灵活的设计，如图 4.2 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-0b90670f746b4c409e93c1a4f489af3b.webp" />   
    </p>
<p>图 4.2 基于神经网络的协同过滤框架</p>
<p>NCF 框架对比第三章所说的 CF 方法最主要引入了 MLP 去拟合 user 和 item 的非线性关系，而不是直接通过 inner product 或者 cosine 去计算两者关系，提升了网络的拟合能力。然而 MLP 对于直接学习和捕获从 mf 提取的 user 和 item vector 能力其实并不强。在 wsdm2018 的一篇文章质疑的就是 MLP 对特征组合的拟合能力。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-29098f997823446d98c1bf31fd0c68c1.jpeg" />   
    </p>
<p>图 4.3 DNN 模型拟合数据实验</p>
<p>该 paper 做了一组实验，使用 1 层的 MLP 网络去拟合数据；实验证明对于二维一阶的数据，也需要 100 个节点才能拟合；如果超过 2 阶，整个 MLP 的表现将会非常差。文章因此说明了 DNN 对于高阶信息的捕捉能力并不强，只能捕捉低阶信息。</p>
<p>下文要讲的模型，也是在模型结构或者特征层面做的各种变化。</p>
<h5 id="4111-neumf-模型--neural-matrix-factorization-">4.1.1.1 NeuMF 模型 ( Neural Matrix Factorization )</h5>
<p>Neural MF，顾名思义，同时利用了 MF 和神经网络 MLP 的能力来拟合 matching score；MF 利用向量内积学习 user 和 item 的关联，同时 MLP 部分捕捉两者的其他高阶信息。这篇 paper 其实和 NCF 框架是出自同一篇 paper 的。模型可以分为 GMF 和 MLP 两个部分来看，如图 4.4 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-ed69472130484ffab26d3ca15ffc3844.webp" />   
    </p>
<p>图 4.4 NeuMF 模型结构框架</p>
<p>❶ GMF ( General Matrix Factorization ) 部分</p>
<p>User 和 item 都通过 one-hot 编码得到稀疏的输入向量，然后通过一个 embedding 层映射为 user vector 和 item vector。这样就获得了 user 和 item 的隐向量，一般可以通过向量点积或者哈达马积 ( element-wide product ) 得到交互，不过在 NeuMF 中多连接了一个连接层，也就是 GMF layer：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-181f64e3d3b34f19a37433a6fa951fea.png" />   
    </p>
<p>❷ MLP 部分</p>
<p>输入和 GMF 部分一样，都是 one-hot 的稀疏编码，然后通过 embedding 层映射为 user vector 和 item vector。注意到这里 user 和 item 的 vector 和 GMF 部分是不一样的，原因是 GMF 和 MLP 两个网络结构对隐层维度要求不同，MLP 部分会高一些 ( 个人感觉 share embedding 能更充分训练 embedding )。</p>
<p>Embedding 层之后就是几层常规的 MLP，这块没什么好说的，最后一层输出作为 MLP 的 output。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-0d881eaedae14c3cbcc692b6dfe91e3f.webp" />   
    </p>
<h5 id="4112-nncf-模型--neighbor-based-ncf-">4.1.1.2 NNCF 模型 ( Neighbor-based NCF )</h5>
<p>CIKM2017 提出的一种基于 neighbor 的 NCF 方法，最大的不同在于输入除了 user 和 item 的信息，还各自引入了 user 和 item 各自的 neighbor 信息。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-8fdbfa2abba54b02a47e187e00379015.webp" />   
    </p>
<p>图 4.5 所示的输入由两部分组成，中间 x_u和 y_i为原始的 user 和 item 的 one-hot 输入，通过 embedding 层后映射为p_u和 q_i的 embedding 向量，然后通过哈达马积作为 MLP 的输入。而输入层两侧的 n_u和 n_i是 user 和 item 各自的 neighbor 信息的输入，这里 n_u和n_1信息如何提取可以采用多种手段，如二部图挖掘，user-CF 或者 item-CF 等。</p>
<p>对于 neighbor 信息，由于每个用户和 item 的 neighbor 数不一致，输入是不定长的，通过卷积和 pooling 后提取得到定长的 embedding，然后和 user 以及 item 本身的向量 concat 后输入到模型中：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-031d45c7d1f3427992a4695a9b1edfe6.webp" />   
    </p>
<h5 id="4113-oncf-模型--outer-product-based-ncf-">4.1.1.3 ONCF 模型 ( Outer-Product based NCF )</h5>
<p>何向南博士 2018 年在 NCF 模型框架上提出了 outer-product based NCF，在原有的 NCF 框架上，引入了 outer product 的概念，如图 4.6 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-4b033877c0dd4f21af528421c81d8874.jpeg" />   
    </p>
<p>图 4.6 ONCF 模型框架</p>
<p>在 embedding layer 之后，O-NCF 模型引入了 interaction map 也就是特征交叉层，对于用户 u 的向量 p_u和物品 i 的向量 q_i，引入两者的 outer-product：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-abbd1ac2737241ca9cc5857abe09d1ee.png" />   
    </p>
<p>E 是一个 k <em>k 维的矩阵，其中的每个 element 两两相乘，得到 2 维的矩阵。到这，可以通过把二维矩阵展开变成一个 k_2 维度的向量，作为 MLP 的输入。假设 k=64，那么 E 就是个 4096 的向量，每一层隐层单元个数设置为上一层的一半，那么第一层的维度为 4096</em> 2048 约需要 840 万的网络参数需要训练，参数量非常巨大。</p>
<p>因此，文章提出了一种利用 CNN 局部连接共享参数的方法来减少 embedding layer 到 hidden layer 之间的参数，如图 4.7 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-cdeae97c2f5142d58b655795ec56fa67.webp" />   
    </p>
<p>图 4.7 ConvNCF 模型框架</p>
<p>假设隐层维度 K=64，有 6 层 hidden layer，每一层有 32 个卷积核 ( feature map )，步长 stride=2，那么经过每个卷积核后的 feature map 大小为原来的 1/4 ( 长和宽各少了一半 )。以第一层卷积为例：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-ab383e3414474cd9a1e96c1fb0c59676.webp" />   
    </p>
<p>那么第一层卷积后得到的网络是个 32 <em>32</em> 32 的 3 维 vector，其中最后一个 32 代表 feature map 个数。这里如何体现特征交叉的思想呢？ei,j,c 代表的就是在前一层的 feature map 中，第 i 个单元和第 j 个 element 的二阶交叉。第一层 feature map 中，每个单元提取的是上一层 2 <em>2 区域的 local 连接信息，第三层提取的就是第一层 4</em> 4 的信息，那么在网络的最后一层就能提取到原始 feature map 里的 global 连接信息，从而达到高阶特征提取的目的。</p>
<p>总结来说，使用原始的 outer-product 思想，在第一层网络处有近千万的参数需要学习，而使用 CNN 网络一方面能够减少参数量，另一方面又同时提取了低阶和高阶特征的组合。个人觉得引入 CNN 固然能节省内存，但也同时会带来训练和推理时间的增加，是一种时间换空间的思想。另外用 CNN 是否能够比原始 MLP 更有效拟合特征组合也需要结合数据分布去看。</p>
<h5 id="4114-小结">4.1.1.4 小结</h5>
<p>基于 NCF 框架的方法基础原理是基于协同过滤，而协同过滤本质上又是在做 user 和 item 的矩阵分解，所以，基于 NCF 框架的方法本质上也是基于 MF 的方法。矩阵分解本质是尽可能将 user 和 item 的 vector，通过各种方法去让 user 和 item 在映射后的空间中的向量尽可能接近 ( 用向量点击或者向量的 cosine 距离直接衡量是否接近）。</p>
<p>而另外一种思路，基于翻译的方法，也叫 translation based model，认为 user 和 item 在新的空间中映射的 vector 可以有 gap，这个 gap 用 relation vector 来表达，也就是让用户的向量加上 relation vector 的向量，尽可能和 item vector 接近。两种方法的区别可以用图 4.8 形象的表示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-3a4f89d5754040bf8cba453919c9b270.jpeg" />   
    </p>
<p>图 4.8 基于矩阵分解和基于翻译的模型区别</p>
<h4 id="412-基于-translation-框架的方法"><strong>4.1.2 基于 translation 框架的方法</strong></h4>
<h5 id="4121-transrec-模型">4.1.2.1 transRec 模型</h5>
<p>2017 年的 recsys 会议上提出的一种基于 &ldquo;translate&rdquo; 的推荐方法，要解决的是 next item 的推荐问题。基本思想是说用户本身的向量，加上用户上一个交互的 item 的向量，应该接近于用户下一个交互的 item 的向量，输入是 (user, prev item, next item)，预测下个 item 被推荐的概率。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-48e3ef6e7806419e8ee5c996c1d1f99e.jpeg" />   
    </p>
<p>图 4.9 transRec 模型框架</p>
<p>用户向量表达如下：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-fe20776a8cd247d5804ef37f7614abbe.webp" />   
    </p>
<p>这里 r_i和 r_j表示的是用户上一个交互的 item i 和下一个交互的 item j，t_u为用户本身的向量表达。而在实际的推荐系统中，往往存在数据稀疏和用户冷启动问题，因此，作者将用户向量 t_u分解成了两个向量：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-58482968058844ee83c4f5dce56fad2a.webp" />   
    </p>
<p>这里 t 可以认为是全局向量，表示的是所有用户的平均行为，t_u表示用户 u 本身的 bias，例如对于冷启动用户，t_u可以设置为 0，用全局用户的表达 t 作为冷启动。</p>
<p>对于热门 item 由于出现次数非常多，会导致最终热门 item 的向量和绝大多数用户向量加上 item 向量很接近，因此文章对热门 item 做了惩罚，最终，已知上一个 item i，用户和下一个 item j 的匹配 score 表达为：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-41d5430bc27142af831464fe5f7697ab.png" />   
    </p>
<p>其中， 第一项 β_j表示的是物品 j 的全局热度；第二项 d 表示的是用户向量加上物品 i 的向量与物品 j 向量的距离；距离越小表示 i 和 j 距离越接近，被推荐的可能性就越大。</p>
<h5 id="4122-lrml-模型--latent-relational-metric-learning-">4.1.2.2 LRML 模型 ( Latent Relational Metric Learning )</h5>
<p>前面讲到，基于 translation 框架的方法对比基于 CF 框架方法最大的不同，在于找到一个 relation vector，使得 user vector + relation vector 尽可能接近 item vector。WWW2018 提出的 LRML 模型通过引入 memory network 来学习度量距离。可以分为三层 layer，分别是 embedding layer, memory layer 和 relation layer。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-d9eac916784d4b0081d95df4e0c6c984.jpeg" />   
    </p>
<p>图 4.10 LRML 模型框架</p>
<p>❶ embedding layer</p>
<p>底层是常规的双塔 embedding，分别是用户 embedding 矩阵和物品的 embedding 矩阵，用户 one-hot 输入和 item 的 one-hot 输入通过 embedding 后得到用户向量 p 和物品向量 q。</p>
<p>❷ memory layer</p>
<p>记忆网络层是文章的核心模块，作者通过引入 memory layer 作为先验模块。这个模块可以分为三个步骤进行计算：</p>
<p>① 用户和物品 embedding 融合</p>
<p>Embedding 层得到的 user 和 item 向量 p 和 q 需要先经过交叉合成一个向量后输入到下一层，作者提到使用哈达码积效果优于 MLP 效果，也更简单：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-1a298ac43aaf42b293c1dc9608e92e0b.png" />   
    </p>
<p>② 用户-物品 key addressing</p>
<p>从第一步得到的向量 s 中，去和 memory 记忆网络模块中的各个 memory vector 挨个计算相似度，相似度可以用内积表达并做归一化：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-32df50f8c1414ae885552329866804fb.webp" />   
    </p>
<p>得到的 a_i代表的是当前用户-物品输入对 (p，q) 与 memory-network 中的第 i 个向量的相似度。</p>
<p>③ 最终加权表达</p>
<p>最终得到的 relation vector 是第二步得到的 memory 记忆网络中不同 vector 的加权表达，如下所示：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-afa5633412e24bf992625d7631aa8325.png" />   
    </p>
<p>❸ relation layer</p>
<p>从 memory layer 得到的 r 向量可以认为是用户向量 p 与物品向量 q 的 relation vector，最终的距离用平方损失衡量，如图 4.11 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-a86d24d0e40c451a9a4c3c97dd9fbbe0.webp" />   
    </p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-22102e715234468985771f73e4fcda81.webp" />   
    </p>
<p>图 4.11 LRML relation 层以及 loss 结构</p>
<p>由于解决的是推荐物品的排序问题，文章使用的是 pairwise loss，因此在网络的最后一层，对 user 和 item 分别进行负样本采样得到 p&rsquo; 和 q&rsquo;，然后使用 pairwise hinge loss 进行优化：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-62a255f2ebc64e529b19e546c18916f4.webp" />   
    </p>
<h3 id="42-feature-based-的深度模型"><strong>4.2 feature-based 的深度模型</strong></h3>
<p>4.1 介绍的基于 CF 的方法，对大多数推荐系统来说，输入的特征向量往往都是非常高维而且稀疏的，而特征之间的交叉关系对模型来说往往都是非常重要的。例如，用户一般会在一天快吃三餐的时候，打开和订餐相关的 app，这样，用户使用订餐 app 和时间存在着二阶交叉关系；又比如说，男性的青年群体，往往更喜欢射击类的游戏，性别、年龄以及类目之间存在着三阶的交叉关系。因此，如何捕捉特征之间的交叉关系，衍生了众多基于特征的模型，在这里将这些捕捉特征交叉关系的模型称为 feature-based model。</p>
<h3 id="421-widedeep-模型"><strong>4.2.1 wide&amp;deep 模型</strong></h3>
<p>提到深度学习模型，最经典的莫过于 2016 年 Google 提出的 wide and deep 模型。说是模型，不如说是通用的一套范式框架，在整个工业界一举奠定了风靡至今的模型框架，如图 4.12 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-09745ecd02394b6ebf4765d1ccad0f1a.jpeg" />   
    </p>
<p>图 4.12 wide&amp;deep 模型框架</p>
<p>在这个经典的 wide&amp;deep 模型中，Google 提出了两个概念：generalization ( 泛化性 ) 和 memory ( 记忆性 )。</p>
<p>❶ 记忆性：wide 部分长处在于学习样本中的高频部分，优点是模型的记忆性好，对于样本中出现过的高频低阶特征能够用少量参数学习；缺点是模型的泛化能力差，例如对于没有见过的 ID 类特征，模型学习能力较差。</p>
<p>❷ 泛化性：deep 部分长处在于学习样本中的长尾部分，优点是泛化能力强，对于少量出现过的样本甚至没有出现过的样本都能做出预测 ( 非零的 embedding 向量 )，容易带来惊喜。缺点是模型对于低阶特征的学习需要用较多参数才能等同 wide 部分效果，而且泛化能力强某种程度上也可能导致过拟合出现 bad case。尤其对于冷启动的一些 item，也有可能用用户带来惊吓。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-14291124375e4d5f8c596c2e978261e0.webp" />   
    </p>
<p>图 4.13 wide&amp;deep 模型特征框架</p>
<p>值得注意的是，虽然模型的 deep 部分拟合和泛化能力很强，但绝对不意味着把特征交叉都交给 MLP 就够了。实际证明，对于重要的一些人工经验的特征，对于提升整体效果还是非常重要的，如图 4.13 所示。这个人工特征的所谓缺点，也是后续各种模型结构想对其进行 &ldquo;自动化&rdquo; 的优化点。</p>
<h4 id="422-deep-crossing-模型"><strong>4.2.2 deep crossing 模型</strong></h4>
<p>微软在 2016 年提出了一套框架 deep crossing，这篇文章在输入到 embedding 这里到是和 wide&amp;deep 没有太多不同，主要区别在于 MLP 部分。</p>
<p>Google 的 wide&amp;deep 模型里深度网络的 MLP 部分是全连接网络，每一层的网络输入都是前一层的输入出，受限于模型结构，越往后越难学习到原始输入的表达，一般深度不会太深，超过 5 层的网络在工业界已经算很少见了。为了解决这个问题，deep crossing 网络引入了 resnet 残差网络的概念，通过 short-cut，在 MLP 的深层网络，也能接收来自第一层的输入，这样可以使得模型的深度达到 10 层之多，如图 4.14 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-0ed5cc752e0c4b8ea7d829686682c5ed.webp" />   
    </p>
<p>图 4.14 deep crossing 模型框架</p>
<p>上述提到的 wide&amp;deep 以及 deep crossing 框架更像是在模型结构做的改进，一个引入了 wide&amp;deep，一个引入了 resnet，特征层面并没有做太多改造，如何体现 feature-base 呢？SigIR2017 就有一篇文章做了个实验，对 wide&amp;deep 以及 Deep&amp;Cross 实验按照 embedding 是否做初始化分别做了实验。实验发现，如果 embedding 是随机初始化的，两个深度模型连基础的 FM 模型都打不过；哪怕经过 FM 初始化了 embedding，wide&amp;deep 模型效果也仅仅略好于 FM 模型，而 deep crossing 模型依然比不过 FM 模型，实验结果如图 4.15 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-a1c5b015235c49578ecc55795a8f5095.jpeg" />   
    </p>
<p>图 4.15 不同初始化对模型影响</p>
<p>这个结论也引出了关于 MLP 的一些思考，全连接网络表面上看对所有节点都进行了连接，理论上应该学习到了各个节点的交叉特征，但是从结果上来看，MLP 对这些特征交叉的学习能力确实非常差的。纠其原因，还是在模型结构的设计上。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-2c8253d270794b60bf66ae5724cefbe5.webp" />   
    </p>
<p>图 4.16 wide&amp;deep 模型和 deep crossing 模型</p>
<p>图 4.16 里无论是 wide&amp;deep 还是 deep crossing network，embedding 层之后到 MLP 之间，都是将 embedding 做 concat 的。这些 concat 后的信息其实能够表达的特征交叉信息其实是非常有限的，仅靠 MLP 想完全捕捉到特征的有效交叉其实是非常困难的。因此，有大量工作关于在 embedding 这里如何捕捉特征交叉，其实就是在 MLP 网络之前，利用更多的数学先验范式做特征交叉去提取特征，这也是本节提到的方法叫做 feature-based 的原因。</p>
<h4 id="423-pnn-模型"><strong>4.2.3 PNN 模型</strong></h4>
<p>Embedding layer 进入 MLP 之前，引入了 product layer 来显式的学习每个 field 的 embedding 向量之间的两两交叉，如图 4.17 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-1fa3e927d1c4421bb1a89a93863dcf9b.webp" />   
    </p>
<p>图 4.17 PNN 模型框架</p>
<p>左边 z 为 embedding 层的线性部分，右边为 embedding 层的特征交叉部分。这种 product 思想来源于，推荐系统中的特征之间的交叉关系更多是一种 and &ldquo;且&rdquo; 的关系，而非 add &ldquo;加&rdquo; 的关系。例如，性别为男且喜欢游戏的人群，比起性别男和喜欢游戏的人群，前者的组合比后者更能体现特征交叉的意义。根据 product 的方式不同，可以分为 inner product ( IPNN ) 和 outer product ( OPNN )，如图 4.18 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-1b80736746dc4499bb1980fdcb4fc7e2.webp" />   
    </p>
<p>图 4.18 PNN 模型的两种不同交叉方式</p>
<p>其中，IPNN 模型每个特征是个 inner product，f 个 field 两两交叉，得到的新的特征组合有 f*(f-1)/2 个；outer product 是两个向量的乘积，得到的新的特征组合有 f*(f-1)/2 <em>k</em> k 个。</p>
<h4 id="424-deepfm-模型"><strong>4.2.4 deepFM 模型</strong></h4>
<p>Google 的 wide&amp;deep 框架固然强大，但由于 wide 部分是个 LR 模型，仍然需要人工特征工程。华为诺亚方舟团队结合 FM 相比 LR 的特征交叉的功能，在 2017 年提出了 deepFM，将 wide&amp;deep 部分的 LR 部分替换成 FM 来避免人工特征工程，如图 4.19 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-d3e5e090a3eb4abeb4731f9bd1c1ff11.webp" />   
    </p>
<p>图 4.19 deepFM 模型框架</p>
<p>比起 wide&amp;deep 的 LR 部分，deeFM 采用 FM 作为 wide 部分的输出，FM 部分如图 4.20 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-7fefae16d39b4f0fb5172a4b5a17d6e0.webp" />   
    </p>
<p>图 4.20 deepFM 模型中的 FM 部分</p>
<p>除此之外，deepFM 还有如下特点：</p>
<p>❶ 更强的低阶特征表达</p>
<p>Wide 部分取代 WDL 的 LR，与 4.2.1 和 4.2.2 提到的 wide&amp;deep 模型以及 deep crossing 模型相比更能捕捉低阶特征信息。</p>
<p>❷ Embedding 层共享</p>
<p>Wide&amp;deep 部分的 embedding 层得需要针对 deep 部分单独设计；而在 deepFM 中，FM 和 DEEP 部分共享 embedding 层，FM 训练得到的参数既作为 wide 部分的输出，也作为 DNN 部分的输入。</p>
<p>❸ end-end 训练</p>
<p>Embedding 和网络权重联合训练，无需预训练和单独训练。</p>
<h4 id="425-nfm-模型--neural-factorization-machines-"><strong>4.2.5 NFM 模型 ( Neural Factorization Machines )</strong></h4>
<p>DeepFM 在 embedding 层后把 FM 部分直接 concat 起来 ( f*k 维，f 个 field，每个 filed 是 k 维向量 ) 作为 DNN 的输入。Neural Factorization Machines，简称 NFM，提出了一种更加简单粗暴的方法，在 embedding 层后，做了一个叫做 Bi-interaction 的操作，让各个 field 做 element-wise 后 sum 起来去做特征交叉，MLP 的输入规模直接压缩到 k 维，和特征的原始维度 ｎ 和特征 field 维度 f 没有任何关系，如图 4.21 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-720a7498472d424081e0c3d0e35af50f.png" />   
    </p>
<p>图 4.21 NFM 模型框架</p>
<p>这里论文只画出了其中的 deep 部分， wide 部分在这里省略没有画出来。Bi-interaction 所做的操作很简单：让 f 个 field 两两 element-wise 相乘后，得到 f*(f-1)/2 个维度为 k 的向量，然后直接 sum 起来，最后得到一个 k 维的向量。所以该层没有任何参数需要学习，同时也降低了网络复杂度，能够加速网络的训练；但同时这种方法也可能带来较大的信息损失。</p>
<h4 id="426-afm-模型--attention-factorization-machines-"><strong>4.2.6 AFM 模型 ( Attention Factorization Machines )</strong></h4>
<p>前面提到的各种网络结构中的 FM 在做特征交叉时，让不同特征的向量直接做交叉，基于的假设是各个特征交叉对结果的贡献度是一样的。这种假设往往不太合理，原因是不同特征对最终结果的贡献程度一般是不一样的。Attention Neural Factorization Machines，简称 AFM 模型，利用了近年来在图像、NLP、语音等领域大获成功的 attention 机制，在前面讲到的 NFM 基础上，引入了 attention 机制来解决这个问题，如图 4.22 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-2f629974408946b89e480141e13d320c.webp" />   
    </p>
<p>图 4.22 AFM 模型框架</p>
<p>AFM 的 embedding 层后和 NFM 一样，先让 f 个 field 的特征做了 element-wise product 后，得到 f*(f-1)/2 个交叉向量。和 NFM 直接把这些交叉项 sum 起来不同，AFM 引入了一个 Attention Net，认为这些交叉特征项每个对结果的贡献是不同的，例如 x_i和 x_j的权重重要度，用 a_{ij}来表示。从这个角度来看，其实 AFM 其实就是个加权累加的过程。Attention Net 部分的权重 a_{ij}不是直接学习，而是通过如下公式表示：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-3907495041a74437a1670e53c26a1a43.webp" />   
    </p>
<p>这里 t 表示 attention net 中的隐层维度，k 和前面一样，为 embedding 层的维度。所以这里需要学习的参数有 3 个，W，b，h，参数个数共 t <em>k+2</em> t 个。得到 a_{ij}权重后，对各个特征两两点积加权累加后，得到一个 k 维的向量，引入一个简单的参数向量 h^T，维度为 k 进行学习，和 wide 部分一起得到最终的 AFM 输出。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-1fd2f5fa1e0f46f3ba0aa2c8f198b668.webp" />   
    </p>
<p>图 4.23 AFM 模型中 attention 的可视化解释</p>
<p>关于 AFM 还有个好处，通过 attention-base pooling 计算的 score 值 a_{ij}体现的是特征v_i和 v_j之间的权重，能够选择有用的二阶特征，如图 4.23 所示。</p>
<h3 id="427-dcn-模型--deep-cross-network-"><strong>4.2.7 DCN 模型 ( Deep Cross Network )</strong></h3>
<p>前面提到的几种 FM-based 的方法都是做的二阶特征交叉，如 PNN 用 product 方式做二阶交叉，NFM 和 AFM 也都采用了 Bi-interaction 的方式学习特征的二阶交叉。对于更高阶的特征交叉，只有让 deep 去学习了。为解决这个问题，Google 在 2017 年提出了 Deep&amp;Cross Network，简称 DCN 的模型，可以任意组合特征，而且不增加网络参数。图 4.24 为 DCN 的结构。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-6f1189a638ac462e8f826fd58d029358.webp" />   
    </p>
<p>图 4.24 DCN 模型结构</p>
<p>整个网络分 4 部分组成：</p>
<p>❶ embedding and stacking layer</p>
<p>之所以不把 embedding 和 stacking 分开来看，是因为很多时候，embedding 和 stacking 过程是分不开的。前面讲到的各种 XX-based FM 网络结构，利用 FM 学到的 v 向量可以很好的作为 embedding。而在很多实际的业务结构，可能已经有了提取到的 embedding 特征信息，例如图像的特征 embedding，text 的特征 embedding，item 的 embedding 等，还有其他连续值信息，例如年龄，收入水平等，这些 embedding 向量 stack 在一起后，一起作为后续网络结构的输入。当然，这部分也可以用前面讲到的 FM 来做 embedding。为了和原始论文保持一致，这里我们假设 x_0向量维度为 d ( 上文的网络结构中为 k )，这一层的做法就是简答的把各种 embedding 向量 concat 起来。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-effdcc851d21494099d5e41953375969.png" />   
    </p>
<p>❷ deep layer network</p>
<p>在 embedding and stacking layer 之后，网络分成了两路，一路是传统的 DNN 结构。表示如下：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-a6ba7f616c4f4d968d9d01ac3e3fdabf.webp" />   
    </p>
<p>为简化理解，假设每一层网络的参数有 m 个，一共有 L_d层，输入层由于和上一层连接，有 d <em>m 个参数 ( d 为x_0向量维度 )，后续的 L_d -1 层，每层需要 m</em>(m+1) 个参数，所以一共需要学习的参数有d*m+m*(m+1)*(L_d - 1)。最后的输出也是个 m 维向量。</p>
<p>❸ cross layer network</p>
<p>Embedding and stacking layer 输入后的另一路就是 DCN 的重点工作了。每一层 l+1 和前一层 l 的关系可以用如下关系表示：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-1ede983de09a41a7bc22b282c35b7d8a.png" />   
    </p>
<p>可以看到 f 是待拟合的函数，x_l即为上一层的网络输入。需要学习的参数为w_l和 b_l，因为 x_l维度为 d，当前层网络输入 x_{l + 1}也为 d 维，待学习的参数w_l和 b_l也都是 d 维向量。因此，每一层都有 2*d 的参数 ( w 和 b ) 需要学习，网络结构如下。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-c15564098cfe4e9d971cdb18024ea638.webp" />   
    </p>
<p>图 4.25 DCN 模型的 cross 原理</p>
<p>经过 L_c层的 cross layer network 后，在该 layer 最后一层 L_c层的输出为L_{c2}的 d 维向量。</p>
<p>❹ combination output layer</p>
<p>经过 cross network 的输出X_{L1} ( d 维 ) 和 deep network 之后的向量输入 ( m 维 ) 直接做 concat，变为一个 d+m 的向量，最后套一个 LR 模型，需要学习参数为 1+d+m。</p>
<p>总结起来，DCN 引入的 cross network 理论上可以表达任意高阶组合，同时每一层保留低阶组合，参数的向量化也控制了模型的复杂度。Cross 网络部分的交叉学习的是特征向量中每一个 element 的交叉，本质上是 bit-wise 的。</p>
<h4 id="428-xdeepfm-模型--extreme-deep-factor-machine-"><strong>4.2.8 xDeepFM 模型 ( extreme Deep Factor Machine )</strong></h4>
<p>xDeepFM 模型从名字上听好像是 deepFM 模型的升级，但其实更应该拿来和 DCN 模型做对比。DCN 模型引入了高阶特征交叉，但是特征的交叉本质上是在 bit-wise 的。而 xDeepFM 模型认为特征向量 i 和特征向量 j 的交叉过程中，i 本身的元素交叉没有意义，提取 i 和 j 的向量交叉才是更有效捕捉特征的方式，也就是 vector-wise 的交叉，整个模型的框架如图 4.26 所示，最核心的模块在于特征交叉的 CIN 模块。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-081b252c0f2942a1a095fa24aa3bd729.webp" />   
    </p>
<p>图 4.26 xDeepFM 模型结构</p>
<p>首先我们来看下整个 CIN 的整体框架图，如图 4.27 所示，假设特征 field 个数是 m，每个 field 的隐层维度为 d，那么原始 embedding 层的大小为 m*d，而 cross network 有H_k层，提取的是特征的交叉。每一层网络在做什么事情呢？就是和第一层 x_0做特征交叉得到新的特征向量后，然后这 H_k层 cross net 得到的特征向量 concat 到一起，作为 MLP 的输入。那么，这里面，每一层的特征 x_k到底是如何输入层x_0发生交互的？</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-14788f735430493e9907b041b2040e31.jpeg" />   
    </p>
<p>图 4.27 CIN 模块结构</p>
<p>以 cross net 的第 k 层和原始输入 x_0为例，我们看下如何提取得到新的特征，图 4.28 是其特征交叉的过程。其中 x_k的维度为H_k * D ，表示的是第 k 层有H_k个 vector，而原始输入x_0的维度为 m*D，表示输入层有 m 个 D 维的 vector。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-0f80c106c2cf47c38008bf8d55bfe1d8.png" />   
    </p>
<p>图 4.28 CIN 模块中特征交叉过程</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-08368e83f8fe4943bf9e37ca474b373d.webp" />   
    </p>
<p>这里 W_{k,h}表示的是第 k 层的第 h 个 vector 的权重，是模型需要学习的参数。整个公式的理解是整个 xDeepFM 理解的关键，我们具体看下发生了什么：</p>
<p>❶ 首先，从前一层的输入 X_{k-1} ( 一共有H_{k-1}个 vector )，取出任意一个 vector；从原始输入 x_0( 一共有 m 个 vector )，取出任意一个 vector，两者两两做哈达码积，可以得到 H_{k-1} * m 个 vector。</p>
<p>❷ 这 H_{k-1} * m个交叉得到的 vector，每个 vector 维度都是 D，我们通过一个 W 矩阵做乘积进行加权求和，相当于是个带权重的 pooling，最终得到加权求和后的 vector X_{k-1}，表示的是第 h 层第 k 个 vector。这里的 W 矩阵就是模型要学习的。</p>
<p>❸ 为什么说是压缩，压缩体现在哪里？还是用图说话，这里我们看下原始论文给出的图示，有助于整个过程的理解。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-e3d4c5705f4d47f8873382872c812a33.webp" />   
    </p>
<p>图 4.29 CIN 模块具体结构</p>
<p>在图 4.29 左图中，我们把 D 看成是原始二维平面的宽度，我们沿着 D 的方向挨个进行计算。先看 x_k向量中 D 的第一维，有H_k个数；x_0向量中 D 的第一维，有 m 个数，让 H_k和 m 两两计算，就可以得到 H_k*m 的一个平面。一直沿着 D 方向，2，3，4，…D，我们就可以得到一个H_k*m*D的三维矩阵，暂且叫做z^{k+1}，注意这个过程只是简单的矩阵计算，没有参数需要学习。</p>
<p>在 4.29 右边的图中，我们开始提取前面 z^{k+1}的信息，还是以 D 方向的第一维为例，一个 m*H_k的平面，乘以一个大小一样的 m*H_k矩阵 W，然后加权求和，就可以得到这个平面最后压缩的一个实数。整个平面被 &ldquo;压缩&rdquo; 成为了一个一维的数。一直沿着 D 方向求解每个平面压缩后的数，就可以得到一个 D 维的向量。</p>
<p>这就是整个 &ldquo;压缩&rdquo; 的取名原因。整个过程非常类似 CNN 的 filter 卷积思想，W 就是卷积核，得到的每个特征映射的值就是 feature map。</p>
<h4 id="429-fgcnn-模型--feature-generate-by-cnn-"><strong>4.2.9 FGCNN 模型 ( Feature Generate by CNN )</strong></h4>
<p>CNN 模型在图像，语音，NLP 领域都是非常重要的特征提取器，原因是对图像、视频、语言来说，存在着很强的 local connection 信息。而在推荐系统中由于特征输入都是稀疏无序的，很难直接利用 CNN 作为特征提取。华为诺亚方舟在 2019 年的 WWW 会议上提出了一种巧妙的利用 CNN 提取特征的方法 FGCNN，整个模型框架如图 4.30 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-eb12fb1e74bd49488048a9731c893602.webp" />   
    </p>
<p>图 4.30 FGCNN 模型框架</p>
<p>其中利用 CNN 提取新特征是整个 paper 的核心模块，具体框架如图 4.31 所示，可以分为 4 个层，卷积层、池化层、重组层以及特征输出层。下面分别介绍这 4 个不同的层，分别看下各自的输入，输出以及具体的网络结构。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-5e2f343899e34ddcb62c6082e54f4357.webp" />   
    </p>
<p>图 4.31 CNN 模块从原始特征提取新特征</p>
<p>❶ 卷积层 Convolution Layer</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-7a91dd99dc484baeb8543d76d44378b1.webp" />   
    </p>
<p>图 4.32 卷积层的特征卷积过程</p>
<p>原始 one-hot 特征经过 embedding 后，进入的是卷积层。卷积的思想和 TextCNN 类似，通过一个高度为h_p，宽度为 d 的卷积核进行卷积；其中高度 h_p代表的是每次卷积连接的邻域的个数，以图 4.32 为例，h_p=2，表示二维特征的交叉，从上到下卷积，表示的是 N&amp;A，A&amp;H，H&amp;G 卷积，分别表示名字和年龄、年龄和身高、身高和性别的交叉；而宽度方向 d 需要和 embedding 的维度 d 保持一致，这和 TextCNN 的思想是一致的，只在高度方向上进行卷积。</p>
<p>① Convolutional layer 输入</p>
<p>特征 one-hot 之后经过 embedding 层后的输出作为卷积层的输入，输入维度为n_f*k。n_f为 field 个数，k 为隐层维度</p>
<p>② Convolutional layer 输出</p>
<p>经过二维平面大小不变，增加第三维卷积核大小，第一层卷积层的输出大小为n_f*k*m_{c^1}，C1 为第 1 个卷积层的卷积个数，以 l=1 为例，C_{:,:,i}表示的是第一层卷积层第 i 个 feature-map，每个 feature map 的大小为n_f*k, C_{p,q,i}表示的是第一层第 i 个 feature map 的坐标为 (p，q) 的元素：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-b120c6bf2a874ab2b6a618e56e86321c.webp" />   
    </p>
<p>❷ 池化层 Pooling Layer</p>
<p>特征经过第一层卷积层之后，从 n_f*k 变成了 n_f*k*m_{c^1}，维度反而扩了 m_{c^1} 倍，这和预期的特征降维不一致，解决办法和 CNN 里常用的套路一样，通过 pooling 做降维。该 paper 使用的是 max pooling，在原先卷积高度 h_p上选择最大值作为卷积后的特征表达，表达如下所示：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-bbbb8add6b984ccdbcc7c6b57f3f22fa.webp" />   
    </p>
<p>① pooling 层输入</p>
<p>卷积层输出的时候在高度方向上选择 max，只有高度方向维度发生变化，卷积输出后维度为n_f/h_p*k*m_{c^1}, h_p为卷积核高度，相当于沿着 field 的方向压缩了h_p倍。</p>
<p>❸ 重组层 Recombination Layer</p>
<p>经过特征卷积层和特征池化层后，可以通过特征重组 Recombination layer 生成新的特征组了。还是以第一层的 Recombination layer 为例，m_{c^1} 为原始 feature map 的个数，m_{r^1} 为新生成的特征 feature map 的个数：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-828795f592814865860b9ab5f2529b4a.webp" />   
    </p>
<p>其中 S^1为池化层的输出，而 WR^1和BR^1为该层待学习的参数。</p>
<p>① Recombination 层输入</p>
<p>卷积层的输出S_1为该层的输入，通过 WR^1的矩阵进行特征的 recombination</p>
<p>② Recombination 层输出</p>
<p>新生成的特征维度为 n_f/h_p*k*m_{r^1}</p>
<p>❹ 特征输出层 Concatenation Layer</p>
<p>第一层 Convolution layer -&gt; pooling layer -&gt; recombination layer 后，生成第一层的新特征R_1，连同第 2，3，4，…，nc 层的新特征R_2,R_3, R_4,&hellip;,R_{nc}层一起 concat 起来，组成新的特征R=(R_1,R_2,&hellip;,R_{nc})。之后 R 可以和原始特征一样做各种可能的交叉，例如和原始特征做二阶交叉等， <a href="https://img.6aiq.com/image-d4d56d547e0145a3891beefd90933d71.webp">https://img.6aiq.com/image-d4d56d547e0145a3891beefd90933d71.webp</a> 。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-43431a8eecb148e1b89f41dd01f8f144.web" />   
    </p>
<p>图 4.33 FGCNN 新特征的生成</p>
<p>① concatenation 层输入</p>
<p>重组层 Recombination layer 输出的新特征R_1, R_2, R_3, R_{nc}，作为 concatenation layer 的输入</p>
<p>② concatenation 层输出</p>
<p>原有的特征 E 和新特征 R 连在一起作为 concatenation 的特征输出，作为 NN 层的输入</p>
<h4 id="4210-fibinet-模型--feature-importance--bilinear-feature-interaction-"><strong>4.2.10 FiBiNet 模型 ( Feature Importance &amp; Bilinear feature Interaction )</strong></h4>
<p>新浪微博张俊林团队 2019 年提出的结合特征重要性 ( Fi ) 和双线性特征交叉 ( Bi ) 的方法，Feature Importance &amp; Bilinear feature Interaction，简称 FiBiNet，其实是两个相对独立的模块，两个模块可以独立套用在其他的网络结构上，整体框架如图 4.34 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-bac1169a64584b7ba7eccc22b2f750ab.jpeg" />   
    </p>
<p>图 4.34 FiBiNet 模型框架</p>
<p>❶ SENet：特征重要性提取层</p>
<p>该层作用主要是提取特征重要性。可以分为三层 Squeeze, Extract, Reweight，从原始的特征e_1, e_2, e_3, &hellip;, e_f提取到新的特征v_1, v_2, v_3,&hellip;,v_f，其中 f 为特征 field 的个数，整体框架如下：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-bd88cde14fb348b8ab3825b52b9d70e8.webp" />   
    </p>
<p>图 4.35 SENet 框架</p>
<p>① Squeeze 层</p>
<p>特征压缩层，对每个特征组中的 embedding 向量进行汇总统计量的 pooling 操作，每个 field 的维度为 k，从中 squeeze 压缩提取最重要的 1 维。典型的方法一般有 Max pooling 或者 average pooling，在更原始的 SENet 里用的是 max pooling，但是在该 paper 里作者提到 average pooling 表现更好，个人其实也倾向于认为 average pooling 直觉上更加 make sense，只用 max pooling 从 d 维特征提取一维信息损失实在太大。当然具体表现也需要结合具体数据实验才知道：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-1d2b829ba84a4bd3b0e0216315f4bb93.webp" />   
    </p>
<p>经过 S 层，第 i 维特征从 e_i变成了 z_i，从维度为 d 的向量变成了一个维度为 1 的实数。整个输入从 f*d 的矩阵变为 f 维的向量。</p>
<p>② Excitation 层</p>
<p>特征激活层，向量先做压缩 r ( r 为压缩的倍数 )，然后扩展恢复，类似 auto-encoder 思路：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-dd4d1f48b0a948688d415f9ace71557c.webp" />   
    </p>
<p>W_1和 w_2为两个转换矩阵，先经过压缩然后恢复，最终得到的 A 还是一个维度为 f 的向量。</p>
<p>③ Reweight 层</p>
<p>经过前面 Squeeze 和 Excitation 得到的 A 相当于是原始特征 E 的权重向量，乘以原始的特征 E 后，可以得到最终加权提取的 V 向量，可以认为是原始特征的重要性表达：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-fb388200742a419eb85ed727c8fa72ed.webp" />   
    </p>
<p>总结来说：SENet，用两层全连接阶层提取每个 field 的特征重要性：</p>
<p>S 层：压缩提取 field 中最重要的特征 ( max 或者 average 特征 )</p>
<p>E 层：对 s 层提取的特征信息进行 auto-encoder 式的信息提取</p>
<p>R 层：SE 提取到的每个 field 特征重要性，进行加权提取</p>
<p>❷ Bilinear 特征交叉层</p>
<p>从 SENet 提取到的新特征 V，还有原始特征 E，可以组合成为新的特征一起输入到模型的 MLP 网络中。但是如前面提到的 MLP 对特征交叉很弱的学习能力一样，本文一样提出了一些做特征交叉的方法。我们回顾下在前面提到的两个向量的交叉，无非是向量内积，得到一个实数；或者是向量的哈达马积分如图 4.36 的 a 所示，得到的还是一个向量，如图中 4.36 的 b 方法所示。这两种方法都是直接计算的，没有新的参数学习。在该 paper 中，作者提出了另一种引入参数的方法，也就是 Bilinear 双线性特征交叉的方法，如图中 4.36 的 c 方法所示：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-762fcccd12304beea8a5e2b948ca70c2.webp" />   
    </p>
<p>图 4.36 不同特征交叉方法</p>
<p>假设特征 field 的个数是 f 个，每个 field 的隐层维度为 d。那么，如何确定需要学习的参数 W 呢？根据 W 的共享情况，可以分为以下三种类型：</p>
<p>① field type</p>
<p>W 在所有特征之间是完全共享的，也就是说在任意两个需要交叉的特征 i 和特征 j 之间(v_i, v_j)共享一个 W，W 的参数维度为 k*k：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-f1f88b39403243588d931f3e2f5b599c.png" />   
    </p>
<p>② field each</p>
<p>对于每个特征 i 来说，都需要学习唯一的一个 W_i矩阵，该 W_i只和左乘的特征v_i有关，和右乘特征v_j无关，一共有 f 个 field，所以 W 的参数维度为 f <em>k</em> k：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-447bc0e851bd4ae0b5279d44cd6e4993.webp" />   
    </p>
<p>③ field-interaction type</p>
<p>对于任意两个需要交叉的特征 v_i和 v_j，都需要学习一个唯一的矩阵W_{ij}, W_{ij} 不仅和左乘的特征有关，也和右乘的特征有关，也就是说，v_i和 v_j的交叉，以及 v_j和 v_i的特征交叉，两者用到的W_{ij}和 W_{ji} 是不一样的，所以 W 的参数维度为 f <em>f</em> k*k：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-66f04c872de94e3295b323bbaaeb5fd0.png" />   
    </p>
<p>当然，以上三种不同的特征类型没有绝对的谁好谁坏，不存在越复杂效果越好的情况，对于简单的数据集，可能 w 共享是最好的，具体还是需要实验才能知道。在文章里用了 criteio 和 avazu 数据集进行实验时，使用不同的模型，三种交叉方法也是各有千秋。</p>
<p>总结本文的两个核心工作，第一个工作，SENet，目的是从一个 f <em>d 的特征向量中，提取得到表达了特征重要性的同样维度为 f</em> d 的新特征向量，该网络结构可以和其他模型结构套用。第二个工作 bi-net，从输入的 embedding 特征中提出了几种不同的特征交叉的方法，也可以给其他网络模型做特征交叉提供一些不同的手段。</p>
<h4 id="4211-autoint-模型--auto-feature-interaction-"><strong>4.2.11 AutoInt 模型 ( Auto Feature Interaction )</strong></h4>
<p>目前为止讲到的模型中，使用到 attention 的是 AFM 模型，而在 AutoInt 文章中，作者除了 attention 机制，还使用了在 transform 中很火的 multi-head 和 self-attention 的概念，整体框架如图 4.37 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-4ac1c7e7efcc4d169625fd2442a2dd20.webp" />   
    </p>
<p>图 4.37 AutoInt 模型框架</p>
<p>文章中的输入没有什么特别需要讲的，就是常规的 one-hot 稀疏的特征经过 embedding 层得到 dense value。这里 embedding 层倒是值得一提，一般在大多数推荐系统里，对于 one-hot 做 embedding 是因为需要转成 dense 的特征，而原本就是 dense 的特征原本就是定长 dense 特征，比较少见到做 embedding 的，而在该文章中将连续值特征和 one-hot 一样去做 embedding 处理。不过这一块不影响 auto-int 部分的理解。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-212ce425632d47faaea5ed82c5fb43ee.png" />   
    </p>
<p>图 4.38 embedding 层输入</p>
<p>该 paper 里提到的 attention 机制，才是文章里的核心亮点，在介绍文章的核心机制之前，先在这里简单介绍下 NLP 里的 attention 里涉及到的 3 个重要的概念，query，keys，value。想象一下，你在搜索引擎输入了一个搜索词，这个搜索词就是 query，假设你搜了 &ldquo;应用宝&rdquo;，然后存在一堆候选结果就叫做 keys，例如 &ldquo;下载 app 的地方&rdquo;，&ldquo;手机应用市场&rdquo;，&ldquo;app store&rdquo;，&ldquo;手机系统&rdquo; 等等，对这里的每个 key，都去计算和候选词 query 的相似度，例如 &ldquo;手机应用市场&rdquo; 对 &ldquo;应用宝&rdquo; 的权重，显然是要高于 &ldquo;手机系统&rdquo;，最终的表达，其实就是每个 keys 的 value 的加权求和，也就是说，谁和 query 的相似度高，在结果中 value 的权重占比就更高。经典的 QKV 结果表达如下：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-5db1ea36c87145bf92f426d0447110db.png" />   
    </p>
<p>回到本文的 attention 机制中，假设特征 field 个数为 M，对于第 m 个 field，在第 h 个 head 空间下的特征表达如下：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-5be4312a51d444f088e2f99b36725ef3.webp" />   
    </p>
<p>图 4.39 特征 m 在空间 h 下的 QKV 结构</p>
<p>❶ 特征 m 在空间 h 下的新映射</p>
<p>每个特征 m，在空间 h 下都有 3 组向量表示，分表代表特征 m 在空间 h 下的 query 向量 Equery、key 向量 Ekey、value 向量 Evalue：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-3a516619b2b84400aef686a8417c6dee.webp" />   
    </p>
<p>这里需要学习的参数为W_{Query}, W_{key}, W_{value}，每个矩阵的维度为 d&rsquo;*d，从而将特征从 d 维映射为 d&rsquo;。W 在所有特征中都是共享的，并不是每组特征都需要学习 3 个 W。</p>
<p>❷ 特征 k 对特征 m 相似度表达，使用向量的点积：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-e34d4636e88c4b1c8e5abd48ef6fa79d.webp" />   
    </p>
<p>❸ 特征 k 对特征 m 的归一化注意力 ( M 个特征做归一化 )</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-29b1dee1d7cb4096af032068cc0d01c3.webp" />   
    </p>
<p>❹ 特征 m 在空间 h 下的加权新特征：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-de2c267164b1473ea9c2333b8251cfac.webp" />   
    </p>
<p>❺ 特征 m 的全空间表达：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-19c5a276f52a4355847d5332378b283d.png" />   
    </p>
<p>有 H 个 head，特征 m 最终的表达为 H 个新特征的 concat，特征长度为 H*d&rsquo;。</p>
<p>❻ 特征 m 最终表达：resnet 保留原始信息</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-9e54ea8b021241afb6381a1291d9bd91.webp" />   
    </p>
<p>作者为了保留更多信息，除了第 5 步得到的 multi-head 的新特征，也将原始的特征 em 引入到进来，其实就是一种 resnet 思路，所以在这里需要有个额外的参数 WRes 需要学习，需要将原始的特征维度 d 映射为和 em 一致的 H*d&rsquo;，所以 WRes 的参数维度为 d&rsquo; <em>H</em> d。</p>
<h4 id="4212-din-模型--deep-interest-network-"><strong>4.2.12 DIN 模型 ( Deep Interest Network )</strong></h4>
<p>4.2.11 提到的 AutoInt 里特征的 attention 机制有个特点，就是在计算特征的重要性的时候，所有特征都有机会成为 query, 将其他特征作为 keys 去计算和当前 query 的重要性从而得到权重的。而提到推荐系统里的 attention 机制，不得不提的就是阿里的这篇 deep interest network 了，简称 DIN。工业界的做法不像学术界，很多模型网络结构优化并不一味的追求模型的复杂和网络结构有多 fancy，每一步背后都有大量的业务思考后沉淀下来的。</p>
<p>阿里这篇 DIN 也如此。在了解 DIN 之前，我们先看下 DIN 前身的模型，GwEN 模型 ( Group-wise Embedding Network，阿里内部称呼 )。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-f0063e205b244a67af78ff49607f8e06.jpeg" />   
    </p>
<p>图 4.40 GwEN 模型结构 ( DIN 的 baseline )</p>
<p>前面讲到的很多模型，输入层都是大规模稀疏特征，经过 embedding 层后输入到 MLP 网络中。这里的一个假设就是，每个 field 都是 one-hot 的，如果不是 one-hot 而是 multi-hot，那么就用 pooling 的方式，如 sum pooling，average pooling，max pooling 等，这样才能保证每个特征 field embedding 都是定长的。DIN 的前身 GwEN 模型也一样，对于 multi-hot 特征的典型代表，用户历史行为，比如用户在电商系统里购买过的商品，往往都是几十几百甚至几千的，需要经过 sum pooling 和其他特征 concat 一起。</p>
<p>而这种数学假设其实往往都是和实际的发生场景不一致的。例如一个女性用户过去在淘宝买过白色针织衫、连衣裙、帽子、高跟鞋、单肩包、洗漱用品等，当前候选商品是一件黑色外套，白色针织衫对黑色外套的权重影响应该更大，洗漱用品权重应该更小。如果将这些历史行为过的商品做 sum pooling，那么无论对于当前推荐什么商品，用户都是一个固定向量的表达，信息损失很大，显然优化空间很大。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-ce011f536acc4bfe97c4ff9fda2f5437.webp" />   
    </p>
<p>图 4.41 DIN 模型框架</p>
<p>针对 sum/average pooling 的缺点，DIN 提出了一种 local activation 的思想，基于一种基本的假设：用户历史不同的行为，对当前不同的商品权重是不一样的。例如用户过去有 a，b，c 三个行为，如果当前商品是 d，那么 a，b，c 的权重可能是 0.8，0.2，0.2；如果是商品 e，那么 a，b，c 的权重可能变成了 0.4，0.8，0.1。也就是说，不同的 query，激发用户历史行为不同的 keys 的权重是不一样的。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-e1d333288144470b9788a351b4d14173.webp" />   
    </p>
<p>① query：用户历史行为，长度为 H，e_1, e_2, &hellip;, e_H表示用户历史行为的向量表示。</p>
<p>② keys：当前候选广告 ( 店铺、类目、或者其他 item 实体 )</p>
<p>关于 DIN 里的 activation weight 还有个可以稍微讲几句的点。两个向量的相似度，在前面讲各种 CF 的方法的时候基本是用的点积或者 cosine，2017 年 DIN 挂在 arXiv 的版本中是使用了两个向量本身以及 concat 后进入 MLP 得到其相似度，2018 发在 KDD 的版本中多了 outer product，以及向量相减，相当于引入和保留了更多特征的信息。另外作者在文章提到为了保持不同历史行为对当前 attention 的影响，权重也不做归一化，这个和原始的 attention 也有所不同。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-6667db96f4b746188ab1d7662859895f.webp" />   
    </p>
<p>图 4.42 DIN 模型中的 Attention Unit</p>
<p>作为工业界的落地实践，阿里在 DIN 上很 &ldquo;克制&rdquo; 的只用了最能表达用户个性化需求的特征&ndash;用户行为 keys，而 query 也是当前候选的商品广告，与线上提升 ctr 的指标更为吻合，对工业界的推荐系统来说借鉴意义还是很大的。当然这不是说之前的其他 attention 机制模型没用，不同的数据集，不同的落地场景需求不一致，也给工业界更多的尝试提供了很多思路的借鉴。</p>
<h4 id="4213-dien-模型--deep-interest-evolution-network-"><strong>4.2.13 DIEN 模型 ( Deep Interest Evolution Network )</strong></h4>
<p>在前面讲到的模型中，所使用的特征都是时间无序的，DIN 也如此，用户的行为特征之间并没有先后顺序，强调的是用户兴趣的多样性。但是实际用户的兴趣应该是在不断进化的，用户越近期的行为，对于预测后续的行为越重要，而用户越早期的行为，对于预测后续行为的权重影响应该小一点。因此，为了捕获用户行为兴趣随时间如何发展变化，在 din 提出一年后，阿里又进一步提出了 DIEN，引入了时间序列概念，深度兴趣进化网络。</p>
<p>DIEN 文章里提到，在以往的推荐模型中存在的序列模型中，主要利用 RNN 来捕获用户行为序列也就是用户历史行为数据中的依赖关系，比对用户行为序列直接做 pooling 要好。但是以往这些模型有两个缺点，第一是直接将 RNN 的隐层作为兴趣表达，而一般来隐层的表达和真正表达的商品 embedding 一般不是等价的，并不能直接反映用户的兴趣；另外一点，RNN 将用户历史行为的每个行为看成等权的一般来说也不合理。整个 DIEN 的整体框架，如图 4.43 所示。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-30fa934dac094f868cb06380542890a2.jpeg" />   
    </p>
<p>图 4.43 DIEN 模型框架</p>
<p>❶ 输入层</p>
<p>和 DIN 的输入一样。按照类型可以分为 4 大类</p>
<p>① 用户画像特征：如年龄、性别、职业等</p>
<p>② context 特征：如网络环境、时间、IP 地址、手机型号等，与 user 以及 item 无关</p>
<p>③ target ad 特征：当前候选广告</p>
<p>④ 用户行为特征：DIEN 里最重要的能体现用户个性化的特征，对于每个用户来说，假设有 T 个历史行为，按照发生的先后顺序依次输入模型</p>
<p>❷ embedding 层</p>
<p>将 one-hot 特征转为 dense 的 embedding 向量</p>
<p>❸ 兴趣抽取层 ( interest extractor layer )</p>
<p>该层的主要作用和 DIN 一样，为了从 embedding 层中抽取出用户的兴趣。该 paper 认为用户当前对候选广告是否感兴趣，是和历史行为 behavior 有关的，所以引入了 GRU 的序列模型来拟合抽取用户兴趣。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-ab69df1b659c47328e13554cb3eac6ac.webp" />   
    </p>
<p>经过 GRU 结构后，商品的 embedding 表达从 e(t) 变成了 h(t)，表示第 t 个行为序列的 embedding 表达。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-402fba3c67dd43a582a2463d6e5d8089.webp" />   
    </p>
<p>图 4.44 DIEN 中的辅助 loss 结构</p>
<p>除了 GRU 结构提取隐层的向量，DIEN 还引入了有监督学习，强行让原始的行为向量 e(t) 和 h(t) 产生交互。如图 4.44 所示，引入了辅助 loss(auxiliary loss)，当前时刻 h(t) 作为输入，下一刻的输入 e(t+1) 认为是正样本 (click)，负样本进行负采样 ( 不等于当前时刻 )；然后让 h(t) 与正负样本分别做向量内积，辅助 loss 定义为：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-98127445a1774418bb63d0438276eca4.webp" />   
    </p>
<p>最终的 loss 表达为：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-f6935f2271614c43b3d083083f5e6db3.png" />   
    </p>
<p>其中 a 为超参数，代表的是辅助 loss 对整体 loss 的贡献。有了这个辅助 loss，t 时刻提取的隐层向量 h(t) 可以比原始的 h(t) 更有助于表达用户兴趣，也可以加速网络的训练过程。</p>
<p>❹ 兴趣进化层 ( interest evolving layer )</p>
<p>理论上来说，h(t) 如果替代 e(t) 作为商品的最终表达其实也是可以的，把用户序列 t=1，2，3，…，T 当成用户的 T 个行为过的商品，然后和当前的候选广告套用 DIN 的 attention 网络去计算每个行为和当前候选广告的权重，最终得到用户的历史行为加权表达也是完全 ok 的。但作者认为用户的行为模式是会发展的，因此引入了第二层 GRU 网络来学习每个历史行为序列和当前候选广告之间的权重。</p>
<p>对于每个历史行为h_t，当前候选广告 e_a，通过 softmax 求出两者的权重。注意这里不是直接向量点击，而是引入了矩阵 W，可以认为是简单的一层全连接网络。</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-4ff1f4191e184af49f9b39e0c5ef97f1.webp" />   
    </p>
<p>如何使用这里学习的 attention 作为兴趣进化层的输入，作者又提出了三种计算方法：</p>
<p>❶ AIGRU ( attention input with GRU )</p>
<p>最基础的版本，兴趣进化层第 t 个行为序列的 input 就是隐层h_t的加权：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-4e098856a677407fab71008b3c0ae8a6.webp" />   
    </p>
<p>作者尝试后发现效果并不好，原因是如果是输入 0，也会参与到隐层h_t的计算和更新，相当于给用户兴趣的提取引入了噪音，不相关的用户行为会干扰模型的学习。</p>
<p>❷ AGRU ( attention base GRU )</p>
<p>这里作者使用了 attention 权重 a_t来取代原始 GRU 中的更新门，表达如下：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-fd48a0c2cb4c488e926bff07369464b0.webp" />   
    </p>
<p>❸ AUGRU ( GRU with attentional update gate )</p>
<p>这里作者依然对原始 GRU 做了改造，公式如下：</p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-1e8e937927cd45d5be746d87d10a17d5.webp" />   
    </p>
<p>
        <img class="mx-auto" alt="" src="https://img.6aiq.com/image-9fbea4186b314529960ab9cd9cce827e.png" />   
    </p>
<p>图 4.45 AUGRU 结构</p>
<p>其中，u_t&rsquo; 引入了 a_t 来取代原有的更新向量 u_t，表达的是当前 u_t&rsquo; 对结果的影响。如果当前权重 a_t 较大，u_t&rsquo; 也较大，当前时刻 h_t&rsquo; 保留更多，上一个时刻 h{t-1} 影响也会少一点。从而整个 AUGRU 结果可以更平滑的学习用户兴趣。</p>
<h3 id="43-feature-based-模型总结"><strong>4.3 Feature-based 模型总结</strong></h3>
<p>Feature-based 的模型主要在于学习特征之间的交叉，是近年来整个推荐系统在排序层面的主流研究方向，按照不同维度，我个人把 4.2 列到的模型分为 4 个类型，同一个模型可能会分到不同的类型里。这里的分类仅仅代表个人的观点，欢迎探讨。</p>
<h4 id="431-基于框架的模型"><strong>4.3.1 基于框架的模型</strong></h4>
<p>① wide&amp;deep 模型</p>
<p>深度加宽度的模型范式，本身并不是一个具体的模型，wide 和 deep 部分可以用任意结构框架，wide 的 baseline 是 LR 模型；deep 的 baseline 是 MLP 模型。</p>
<p>② deep crossing 模型</p>
<p>和 wide&amp;deep 最大的不同是 deep 部分引用了 res-net，但个人觉得在目前主流的模型里用的较少。虽然 res-net 理论上可以使用更深的模型提升效果，但在目前工业界大规模稀疏的推荐系统里，还没见到太多往 res-net 方向取得较大进展的工作。</p>
<h4 id="432-基于-fm-特征二阶组合的模型"><strong>4.3.2 基于 FM 特征二阶组合的模型</strong></h4>
<p>学习特征交叉的主要手段是将特征的 embedding 做交叉，特点是特征的交叉是二维的，无非是二阶交叉如何做。</p>
<p>① deepFM 模型</p>
<p>特征交叉使用的 element-wise product，最终得到的是一个实数。</p>
<p>② NFM 模型</p>
<p>交叉使用的 bi-interaction，可以理解成是所有 vector 的 sum pooling，最终得到的是一个向量。</p>
<p>③ AFM 模型</p>
<p>交叉使用的带权重的 bi-interaction，可以理解成所有 vector 的 weight sum pooling，然后使用一个简单的线性模型得到最终的值。</p>
<p>④ PNN 模型</p>
<p>PNN 模型放到基于 FM 的模型是因为本质上和 FM 一样，都是在学习特征的二阶组合。和 deepFM 不同的是，以 IPNN 为例，PNN 的所有特征两两 product 交叉学习得到的值 conat 后得到现实的 product vector 后进入 MLP 模型；而 deepFM 是直接将 FM 模型的 vector 输入到 MLP 模型中。</p>
<h4 id="433-基于-attention-的模型"><strong>4.3.3 基于 attention 的模型</strong></h4>
<p>① AutoInt 模型</p>
<p>使用 multi head 机制，每个特征用 self-attention 得到其他特征和自己的 attention 权重。每个特征的所有 head 得到的特征 concat 起来作为新特征。</p>
<p>② DIN 模型</p>
<p>只使用户历史行为特征作为 keys，keys 之前没有时间序列；得到 keys 和当前候选 item 的 attention。</p>
<p>③ DIEN 模型</p>
<p>只使用用户历史行为特征作为 keys，keys 之间具有先后顺序，引入两层 GRU 表达，第一层 GRU 学习用户历史行为序列的信息，每个时刻 t 输出的隐层 embedding 为该时刻 item 的 embedding 表达；第二层 GRU 用来学习历史每个时刻 t 的历史行为和当前候选广告的权重。</p>
<p>④ FiBiNet 模型</p>
<p>把 FiBiNet 模型放在 attention 模型主要是它的 SENet 部分，通过 squeeze -&gt; Excitation -&gt; reweight 提取原始 embedding 的特征重要性，得到新特征，这里其实也体现了每个特征 embedding 的 attention。</p>
<h4 id="434-基于特征高阶组合的模型"><strong>4.3.4 基于特征高阶组合的模型</strong></h4>
<p>① DCN 模型</p>
<p>使用多层的 cross 来做特征交叉，对于 cross 网络中每一层的输入都由前一层以及第一层的输入组成，从这个维度上代表的是高阶特征的组合。比如说，第四层网络的输出包含了第三层和第二层的输入；而第三层又包含了第二层和第一层，因此这里就是个 3 阶的特征交叉。特征的交叉使用的是 bit-wise，也就是每个特征内部 embedding 的 element 也会两两交叉。</p>
<p>② xDeepFM 模型</p>
<p>使用 CIN 模型来提取特征交叉。和 DCN 模型一样的是，这里也使用了多层的 cross，每一层的输入也是由第一层和上一层的输入组成，不同的是，xdeepFM 模型的特征交叉是 vector wise 层级的，而 DCN 模型是 bit-wise 的。</p>
<p>③ FGCNN 模型</p>
<p>通过使用 CNN 模块，先是卷积层提取局部特征的连接，如高度等于 3 能够提取相邻 3 个 field 的特征的关系，因此具有高阶特征的交叉能力。然后又通过池化层提取 global 信息，确保特征的输入顺序对结果的影响能够被捕捉到。</p>
<h2 id="总结"><strong>总结</strong></h2>
<p>推荐和搜索的本质其实都是匹配，前者匹配用户和物品；后者匹配 query 和 doc。具体到匹配方法，分为传统模型和深度模型两大类，第二章讲的是传统模型，第三章和第四章讲的是深度模型。</p>
<p>对于传统模型，主要分为基于协同过滤的模型和基于特征的模型，两者最大的不同在于是否使用了 side information。基于协同过滤的模型，如 CF，MF，FISM，SVD++，只用到了用户-物品的交互信息，如 userid， itemid， 以及用户交互过的 item 集合本身来表达。而基于特征的模型以 FM 为例，主要特点是除了用户-物品的交互之外，还引入了更多的 side information。FM 模型是很多其他模型的特例，如 MF，SVD++，FISM 等。</p>
<p>对于深度模型，主要分为基于 representation learning 的深度模型以及 match function learning 的深度模型。基于 representation learning 的深度模型学习的是用户和物品的表示，然后通过匹配函数来计算，这里重点在与 representation learning 阶段，可以通过 CNN 网络，auto-encoder，知识图谱等模型结构来学习。</p>
<p>对于 match function learning 的深度模型，也分为基于协同过滤的模型和基于特征的模型。前者和传统 CF 模型一样，不同在于后面接入了 MLP 模型来增强非线性表达，目的是为了使得 user 和 item 的 vector 尽可能接近，这种方法就是基于 NCF 的模型；也有通过引入 relation vector 来是的 user vector 加上 relation vector 后接近 item vector，这种方法是基于翻译的模型。</p>
<p>对于 match function learning 另一种模型框架，是基于特征层面的，有基于 fm 模型的，基于 attention 的，以及高阶特征捕捉的，另外还有基于时间序列的文章中只提到了 DIEN 模型。</p>
<p>整理本篇综述主要基于原始 slides，对其中的 paper 部分粗读部分精读，收获颇多，在全文用如何做好推荐 match 的思路，将各种方法尽可能串到一起，主要体现背后一致的思想指导。多有错漏，欢迎批评指出。</p>
<p><strong>参考资料</strong></p>
<ol>
<li><a href="https://www.comp.nus.edu.sg/~xiangnan/sigir18-deep.pdf">https://www.comp.nus.edu.sg/~xiangnan/sigir18-deep.pdf</a></li>
<li>Xiangnan He, Hanwang Zhang, Min-Yen Kan, and Tat-Seng Chua. Fast matrix factorization for online recommendation with implicit feedback. In SIGIR 2016.</li>
<li>Yehuda Koren, and Robert Bell. Advances in collaborative filtering. Recommender systems handbook. Springer, Boston, MA, 2015. 77-118.</li>
<li>Santosh Kabbur, Xia Ning, and George Karypis. Fism: factored item similarity models for top-n recommender systems. In KDD 2013.</li>
<li>Yehuda Koren. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In KDD 2018.</li>
<li>Steffen Rendle. Factorization machines. In ICDM 2010.</li>
<li>Hong-Jian Xue, Xin-Yu Dai, Jianbing Zhang, Shujian Huang, and Jiajun Chen. Deep matrix factorization models for recommender systems. IJCAI 2017.</li>
<li>Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. Autorec: Autoencoders meet collaborative filtering. In WWW 2015.</li>
<li>Yao Wu, Christopher DuBois, Alice X. Zheng, and Martin Ester. Collaborative denoising auto- encoders for top-n recommender systems. In WSDM 2016.</li>
<li>Sheng Li, Jaya Kawale, and Yun Fu. Deep collaborative filtering via marginalized denoising auto- encoder. In CIKM 2015.</li>
<li>Xue Geng, Hanwang Zhang, Jingwen Bian, and Tat-Seng Chua. Learning image and user features for recommendation in social networks. In ICCV 2015.</li>
<li>Jingyuan Chen, Hanwang Zhang, Xiangnan He, Liqiang Nie, Wei Liu, and Tat-Seng Chua. Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention. In SIGIR 2017.</li>
<li>Fuzheng, Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma. Collaborative knowledge base embedding for recommender systems. In KDD 2016.</li>
<li>Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural collaborative filtering. In WWW 2017.</li>
<li>Ting Bai, Ji-Rong Wen, Jun Zhang, and Wayne Xin Zhao. A Neural Collaborative Filtering Model with Interaction-based Neighborhood. C</li>
</ol>

        </div>

        
<div class="post-archive">
    <ul class="post-copyright">
        <li><strong>原文作者：</strong><a rel="author" href="https://geek.zshipu.com/">知识铺</a></li>
        <li style="word-break:break-all"><strong>原文链接：</strong><a href="https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%B9%B2%E8%B4%A7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B/">https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%B9%B2%E8%B4%A7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B/</a></li>
        <li><strong>版权声明：</strong>本作品采用<a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议</a>进行许可，非商业转载请注明出处（作者，原文链接），商业转载请联系作者获得授权。</li>
        <li><strong>免责声明：</strong>本页面内容均来源于站内编辑发布，部分信息来源互联网，并不意味着本站赞同其观点或者证实其内容的真实性，如涉及版权等问题，请立即联系客服进行更改或删除，保证您的合法权益。转载请注明来源，欢迎对文章中的引用来源进行考证，欢迎指出任何有错误或不够清晰的表达。也可以邮件至 sblig@126.com</li>
    </ul>
</div>
<br/>



        

<div class="post-archive">
    <h2>See Also</h2>
    <ul class="listing">
        
        <li><a href="/post/%E4%BA%92%E8%81%94%E7%BD%91/%E4%B8%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%8D%81%E5%A4%A7%E5%B7%A5%E7%A8%8B%E9%97%AE%E9%A2%98/">下深度学习推荐系统的十大工程问题</a></li>
        
        <li><a href="/post/%E4%BA%92%E8%81%94%E7%BD%91/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E9%81%87%E4%B8%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BA%8C%E5%8D%81%E5%9B%9B%E6%B7%B1%E5%BA%A6%E5%85%B4%E8%B6%A3%E8%BF%9B%E5%8C%96%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E6%88%98/">推荐系统遇上深度学习二十四深度兴趣进化网络原理及实战</a></li>
        
        <li><a href="/post/%E4%BA%92%E8%81%94%E7%BD%91/%E7%8B%AC%E5%AE%B6%E6%8F%AD%E7%A7%98%E4%BA%BF%E7%94%A8%E6%88%B7%E7%9A%84%E7%BE%8E%E5%9B%A2%E6%99%BA%E8%83%BD%E6%8E%A8%E8%8D%90%E5%B9%B3%E5%8F%B0%E6%98%AF%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E7%9A%84/">独家揭秘亿用户的美团智能推荐平台是如何构建的</a></li>
        
        <li><a href="/post/%E4%BA%92%E8%81%94%E7%BD%91/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E9%81%87%E4%B8%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%81%E5%9B%9B/">推荐系统遇上深度学习十四</a></li>
        
        <li><a href="/post/%E4%BA%92%E8%81%94%E7%BD%91/%E4%BA%BF%E5%B9%B4%E5%A4%A9%E7%8C%AB%E5%8F%8C%E6%88%90%E4%BA%A4%E6%80%BB%E9%A2%9D%E6%98%AF%E8%BF%99%E6%A0%B7%E9%A2%84%E6%B5%8B%E7%9A%84/">亿年天猫双成交总额是这样预测的</a></li>
        
    </ul>
</div>


        <div class="post-meta meta-tags">
            
            <ul class="clearfix">
                
                <li><a href='/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F'>推荐系统</a></li>
                
                <li><a href='/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0'>深度学习</a></li>
                
                <li><a href='/tags/%E7%AE%97%E6%B3%95'>算法</a></li>
                
                <li><a href='/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0'>机器学习</a></li>
                
            </ul>
            
        </div>
    </article>
    
    

    
    
    <div class="post bg-white">
      <script src="https://utteranc.es/client.js"
            repo= "zshipu/zshipu-geek"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
      </script>
    </div>
    
</div>

                    <footer id="footer">
    <div>
        &copy; 2023 <a href="https://geek.zshipu.com/">知识铺的博客 By 知识铺</a>
        
        | <a rel="nofollow" target="_blank" href="https://beian.miit.gov.cn/">浙 ICP 备19032823号-1</a>
        
    </div>
    <br />
    <div>
        <div class="github-badge">
            <a href="https://gohugo.io/" target="_black" rel="nofollow"><span class="badge-subject">Powered by</span><span class="badge-value bg-blue">Hugo</span></a>
        </div>
        <div class="github-badge">
            <a href="https://www.flysnow.org/" target="_black"><span class="badge-subject">Design by</span><span class="badge-value bg-brightgreen">飞雪无情</span></a>
        </div>
        <div class="github-badge">
            <a href="https://github.com/flysnow-org/maupassant-hugo" target="_black"><span class="badge-subject">Theme</span><span class="badge-value bg-yellowgreen">Maupassant</span></a>
        </div>
    </div>
</footer>


    
    <script type="text/javascript">
        window.MathJax = {
            tex2jax: {
                inlineMath: [['$', '$']],
                processEscapes: true
                }
            };
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

<a id="rocket" href="#top"></a>
<script type="text/javascript" src='/js/totop.js?v=0.0.0' async=""></script>



    <script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>




                </div>

                <div id="secondary">
    <section class="widget">
        <form id="search" action='https://geek.zshipu.com/search/' method="get" accept-charset="utf-8" target="_blank" _lpchecked="1">
      
      <input type="text" name="q" maxlength="20" placeholder="Search">
      <input type="hidden" name="sitesearch" value="https://geek.zshipu.com/">
      <button type="submit" class="submit icon-search"></button>
</form>
    </section>
    
    <section class="widget">
        <h3 class="widget-title">最近文章</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://geek.zshipu.com/post/java/JavaScript-%E5%8E%8B%E7%BC%A9-Java-%E8%A7%A3%E5%8E%8B%E7%BC%A9/" title="JavaScript 中压缩数据，然后在 Java 中解压缩">JavaScript 中压缩数据，然后在 Java 中解压缩</a>
    </li>
    
    <li>
        <a href="https://geek.zshipu.com/post/java/%E6%8F%AD%E7%A7%98-Java-Record%E6%9B%B4%E5%A5%BD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/" title="揭秘 Java Record：更好的数据处理">揭秘 Java Record：更好的数据处理</a>
    </li>
    
    <li>
        <a href="https://geek.zshipu.com/post/bi/flink/2077-%E7%AC%AC42%E8%AE%B2Flink-%E9%9D%A2%E8%AF%95-%E6%96%B9%E6%A1%88%E8%AE%BE%E8%AE%A1%E7%AF%87/" title="Flink系列-第42讲：Flink 面试-方案设计篇">Flink系列-第42讲：Flink 面试-方案设计篇</a>
    </li>
    
    <li>
        <a href="https://geek.zshipu.com/post/bi/flink/2076-%E7%AC%AC41%E8%AE%B2Flink-%E9%9D%A2%E8%AF%95-%E6%BA%90%E7%A0%81%E7%AF%87/" title="Flink系列-第41讲：Flink 面试-源码篇">Flink系列-第41讲：Flink 面试-源码篇</a>
    </li>
    
    <li>
        <a href="https://geek.zshipu.com/post/bi/flink/2075-%E7%AC%AC40%E8%AE%B2Flink-%E9%9D%A2%E8%AF%95-%E8%BF%9B%E9%98%B6%E7%AF%87/" title="Flink系列-第40讲：Flink 面试-进阶篇">Flink系列-第40讲：Flink 面试-进阶篇</a>
    </li>
    
    <li>
        <a href="https://geek.zshipu.com/post/bi/flink/2074-%E7%AC%AC39%E8%AE%B2Flink-%E9%9D%A2%E8%AF%95-%E5%9F%BA%E7%A1%80%E7%AF%87/" title="Flink系列-第39讲：Flink 面试-基础篇">Flink系列-第39讲：Flink 面试-基础篇</a>
    </li>
    
    <li>
        <a href="https://geek.zshipu.com/post/bi/flink/2073-%E7%AC%AC38%E8%AE%B2Flink-%E8%B0%83%E7%94%A8-CEP-%E5%AE%9E%E7%8E%B0%E6%8A%A5%E8%AD%A6%E5%8A%9F%E8%83%BD/" title="Flink系列- 第38讲：Flink 调用 CEP 实现报警功能">Flink系列- 第38讲：Flink 调用 CEP 实现报警功能</a>
    </li>
    
    <li>
        <a href="https://geek.zshipu.com/post/bi/flink/2072-%E7%AC%AC37%E8%AE%B2%E8%87%AA%E5%AE%9A%E4%B9%89-Pattern-%E5%92%8C%E6%8A%A5%E8%AD%A6%E8%A7%84%E5%88%99/" title="Flink系列- 第37讲：自定义 Pattern 和报警规则">Flink系列- 第37讲：自定义 Pattern 和报警规则</a>
    </li>
    
    <li>
        <a href="https://geek.zshipu.com/post/bi/flink/2071-%E7%AC%AC36%E8%AE%B2%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B6%88%E6%81%AF%E4%BA%8B%E4%BB%B6/" title="Flink系列-第36讲：自定义消息事件">Flink系列-第36讲：自定义消息事件</a>
    </li>
    
    <li>
        <a href="https://geek.zshipu.com/post/bi/flink/2070-%E7%AC%AC35%E8%AE%B2%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF%E5%92%8C-Flink-CEP-%E7%AE%80%E4%BB%8B/" title="Flink系列-第35讲：项目背景和 Flink CEP 简介">Flink系列-第35讲：项目背景和 Flink CEP 简介</a>
    </li>
    
</ul>
    </section>

    
<section class="widget">
    <h3 class="widget-title" style="color:red">福利派送</h3>
    <ul class="widget-list">
        
        <li>
            <a href="https://promotion.aliyun.com/ntms/yunparter/invite.html?source=5176.11533457&amp;userCode=tzm8r4hc" title="【2019双12】ALL IN CLoud 低至1折" target="_blank" style="color:red">
                
                    <img src="https://img.alicdn.com/tfs/TB1_rYHo7P2gK0jSZPxXXacQpXa-690-388.jpg">
                
            </a>
        </li>
        
        <li>
            <a href="https://promotion.aliyun.com/ntms/yunparter/invite.html?source=5176.11533457&amp;userCode=tzm8r4hc" title="助力产业智慧升级，云服务器首年88元起，更有千元代金券礼包免费领！" target="_blank" style="color:red">
                
                    <img src="https://upload-dianshi-1255598498.file.myqcloud.com/345-7c71532bd4935fbdd9a67c1a71e577b1767b805c.200%E7%89%88%E6%9C%ACB.jpg">
                
            </a>
        </li>
        
        <li>
            <a href="https://promotion.aliyun.com/ntms/yunparter/invite.html?source=5176.11533457&amp;userCode=tzm8r4hc" title="【渠道专享低折扣】11月特惠 限时2折" target="_blank" style="color:red">
                
                    <img src="https://img.alicdn.com/tfs/TB1hblJl7Y2gK0jSZFgXXc5OFXa-750-400.jpg">
                
            </a>
        </li>
        
    </ul>
</section>


    <section class="widget">
        <h3 class="widget-title"><a href='/categories/'>分类</a></h3>
<ul class="widget-list">
    
    <li><a href="https://geek.zshipu.com/categories/flutter/">flutter (30)</a></li>
    
    <li><a href="https://geek.zshipu.com/categories/iOS/">iOS (7)</a></li>
    
    <li><a href="https://geek.zshipu.com/categories/unix/">unix (9)</a></li>
    
    <li><a href="https://geek.zshipu.com/categories/%E7%AE%97%E6%B3%95/">算法 (3)</a></li>
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title"><a href='/tags/'>标签</a></h3>
<div class="tagcloud">
    
    <a href="https://geek.zshipu.com/tags/360%E6%90%9C%E7%B4%A2/">360搜索</a>
    
    <a href="https://geek.zshipu.com/tags/58%E5%90%8C%E5%9F%8E/">58同城</a>
    
    <a href="https://geek.zshipu.com/tags/AB%E6%B5%8B%E8%AF%95/">AB测试</a>
    
    <a href="https://geek.zshipu.com/tags/AFM%E6%A8%A1%E5%9E%8B/">AFM模型</a>
    
    <a href="https://geek.zshipu.com/tags/AI/">AI</a>
    
    <a href="https://geek.zshipu.com/tags/AILab/">AILab</a>
    
    <a href="https://geek.zshipu.com/tags/AI%E5%B9%B3%E5%8F%B0/">AI平台</a>
    
    <a href="https://geek.zshipu.com/tags/AKF%E6%9E%B6%E6%9E%84/">AKF架构</a>
    
    <a href="https://geek.zshipu.com/tags/ANN/">ANN</a>
    
    <a href="https://geek.zshipu.com/tags/AOF/">AOF</a>
    
    <a href="https://geek.zshipu.com/tags/AQS/">AQS</a>
    
    <a href="https://geek.zshipu.com/tags/ASR/">ASR</a>
    
    <a href="https://geek.zshipu.com/tags/AUC/">AUC</a>
    
    <a href="https://geek.zshipu.com/tags/AdaBoost/">AdaBoost</a>
    
    <a href="https://geek.zshipu.com/tags/AdaDeltaW/">AdaDeltaW</a>
    
    <a href="https://geek.zshipu.com/tags/AdamW/">AdamW</a>
    
    <a href="https://geek.zshipu.com/tags/Airbnb/">Airbnb</a>
    
    <a href="https://geek.zshipu.com/tags/Alink/">Alink</a>
    
    <a href="https://geek.zshipu.com/tags/Announcement/">Announcement</a>
    
    <a href="https://geek.zshipu.com/tags/ApacheFlink/">ApacheFlink</a>
    
    <a href="https://geek.zshipu.com/tags/AresDB/">AresDB</a>
    
    <a href="https://geek.zshipu.com/tags/Augur/">Augur</a>
    
    <a href="https://geek.zshipu.com/tags/AutoML/">AutoML</a>
    
    <a href="https://geek.zshipu.com/tags/Automaton/">Automaton</a>
    
    <a href="https://geek.zshipu.com/tags/BERT/">BERT</a>
    
    <a href="https://geek.zshipu.com/tags/BI/">BI</a>
    
    <a href="https://geek.zshipu.com/tags/BI%E5%B9%B3%E5%8F%B0/">BI平台</a>
    
    <a href="https://geek.zshipu.com/tags/BPR/">BPR</a>
    
    <a href="https://geek.zshipu.com/tags/Bagging/">Bagging</a>
    
    <a href="https://geek.zshipu.com/tags/Bandits/">Bandits</a>
    
    <a href="https://geek.zshipu.com/tags/BigGAN/">BigGAN</a>
    
    <a href="https://geek.zshipu.com/tags/CNN/">CNN</a>
    
    <a href="https://geek.zshipu.com/tags/CRF/">CRF</a>
    
    <a href="https://geek.zshipu.com/tags/CTR/">CTR</a>
    
    <a href="https://geek.zshipu.com/tags/CTR%E6%A8%A1%E5%9E%8B/">CTR模型</a>
    
    <a href="https://geek.zshipu.com/tags/CTR%E9%A2%84%E4%BC%B0/">CTR预估</a>
    
    <a href="https://geek.zshipu.com/tags/CV/">CV</a>
    
    <a href="https://geek.zshipu.com/tags/CVPR/">CVPR</a>
    
    <a href="https://geek.zshipu.com/tags/ClickHouse/">ClickHouse</a>
    
    <a href="https://geek.zshipu.com/tags/Condition/">Condition</a>
    
    <a href="https://geek.zshipu.com/tags/DDD/">DDD</a>
    
    <a href="https://geek.zshipu.com/tags/DDD%E5%AE%9E%E6%88%98/">DDD实战</a>
    
    <a href="https://geek.zshipu.com/tags/DIN/">DIN</a>
    
    <a href="https://geek.zshipu.com/tags/DKN%E6%A8%A1%E5%9E%8B/">DKN模型</a>
    
    <a href="https://geek.zshipu.com/tags/DMP%E5%B9%B3%E5%8F%B0/">DMP平台</a>
    
    <a href="https://geek.zshipu.com/tags/DPP/">DPP</a>
    
    <a href="https://geek.zshipu.com/tags/DRN/">DRN</a>
    
    <a href="https://geek.zshipu.com/tags/DSP/">DSP</a>
    
    <a href="https://geek.zshipu.com/tags/DSSM/">DSSM</a>
    
    <a href="https://geek.zshipu.com/tags/DeepFFM/">DeepFFM</a>
    
    <a href="https://geek.zshipu.com/tags/DeepFM/">DeepFM</a>
    
    <a href="https://geek.zshipu.com/tags/DeepFM%E6%A8%A1%E5%9E%8B/">DeepFM模型</a>
    
    <a href="https://geek.zshipu.com/tags/DevOps/">DevOps</a>
    
    <a href="https://geek.zshipu.com/tags/DevOps/">DevOps</a>
    
    <a href="https://geek.zshipu.com/tags/Dgraph/">Dgraph</a>
    
    <a href="https://geek.zshipu.com/tags/Doris/">Doris</a>
    
    <a href="https://geek.zshipu.com/tags/Druid/">Druid</a>
    
    <a href="https://geek.zshipu.com/tags/EE%E9%97%AE%E9%A2%98/">EE问题</a>
    
    <a href="https://geek.zshipu.com/tags/ELK/">ELK</a>
    
    <a href="https://geek.zshipu.com/tags/ELMo/">ELMo</a>
    
    <a href="https://geek.zshipu.com/tags/ESSM/">ESSM</a>
    
    <a href="https://geek.zshipu.com/tags/ETL/">ETL</a>
    
    <a href="https://geek.zshipu.com/tags/Embedding/">Embedding</a>
    
    <a href="https://geek.zshipu.com/tags/Epoll/">Epoll</a>
    
    <a href="https://geek.zshipu.com/tags/FFM/">FFM</a>
    
    <a href="https://geek.zshipu.com/tags/FFM%E6%A8%A1%E5%9E%8B/">FFM模型</a>
    
    <a href="https://geek.zshipu.com/tags/FM/">FM</a>
    
    <a href="https://geek.zshipu.com/tags/FM%E6%A8%A1%E5%9E%8B/">FM模型</a>
    
    <a href="https://geek.zshipu.com/tags/FST/">FST</a>
    
    <a href="https://geek.zshipu.com/tags/FTRL/">FTRL</a>
    
    <a href="https://geek.zshipu.com/tags/Faraday/">Faraday</a>
    
    <a href="https://geek.zshipu.com/tags/Feed%E6%B5%81/">Feed流</a>
    
    <a href="https://geek.zshipu.com/tags/FixMatch/">FixMatch</a>
    
    <a href="https://geek.zshipu.com/tags/FixedBitSet/">FixedBitSet</a>
    
    <a href="https://geek.zshipu.com/tags/Flink/">Flink</a>
    
    <a href="https://geek.zshipu.com/tags/FreeWheel/">FreeWheel</a>
    
    <a href="https://geek.zshipu.com/tags/FullGC/">FullGC</a>
    
    <a href="https://geek.zshipu.com/tags/GAN/">GAN</a>
    
    <a href="https://geek.zshipu.com/tags/GBDT/">GBDT</a>
    
    <a href="https://geek.zshipu.com/tags/GBDT&#43;LR%E8%9E%8D%E5%90%88/">GBDT&#43;LR融合</a>
    
    <a href="https://geek.zshipu.com/tags/GBM/">GBM</a>
    
    <a href="https://geek.zshipu.com/tags/GC/">GC</a>
    
    <a href="https://geek.zshipu.com/tags/GNN/">GNN</a>
    
    <a href="https://geek.zshipu.com/tags/GRU4REC/">GRU4REC</a>
    
    <a href="https://geek.zshipu.com/tags/Git/">Git</a>
    
    <a href="https://geek.zshipu.com/tags/Google/">Google</a>
    
    <a href="https://geek.zshipu.com/tags/GraphScope/">GraphScope</a>
    
    <a href="https://geek.zshipu.com/tags/Ha3/">Ha3</a>
    
    <a href="https://geek.zshipu.com/tags/Hbase/">Hbase</a>
    
    <a href="https://geek.zshipu.com/tags/Hologres/">Hologres</a>
    
    <a href="https://geek.zshipu.com/tags/Hystrix/">Hystrix</a>
    
    <a href="https://geek.zshipu.com/tags/IM/">IM</a>
    
    <a href="https://geek.zshipu.com/tags/IRGAN/">IRGAN</a>
    
    <a href="https://geek.zshipu.com/tags/IT%E5%8D%9A%E5%A3%AB/">IT博士</a>
    
    <a href="https://geek.zshipu.com/tags/IT%E7%A7%BB%E6%B0%91/">IT移民</a>
    
    <a href="https://geek.zshipu.com/tags/Iceberg/">Iceberg</a>
    
    <a href="https://geek.zshipu.com/tags/ImageNet/">ImageNet</a>
    
    <a href="https://geek.zshipu.com/tags/Impala/">Impala</a>
    
    <a href="https://geek.zshipu.com/tags/InnoDB/">InnoDB</a>
    
    <a href="https://geek.zshipu.com/tags/IntBlockPool/">IntBlockPool</a>
    
    <a href="https://geek.zshipu.com/tags/js/">js</a>
    
    <a href="https://geek.zshipu.com/tags/JanusGraph/">JanusGraph</a>
    
    <a href="https://geek.zshipu.com/tags/java/">java</a>
    
    <a href="https://geek.zshipu.com/tags/JavaScript/">JavaScript</a>
    
    <a href="https://geek.zshipu.com/tags/KBQA/">KBQA</a>
    
    <a href="https://geek.zshipu.com/tags/KV%E5%AD%98%E5%82%A8/">KV存储</a>
    
    <a href="https://geek.zshipu.com/tags/Kubernetes/">Kubernetes</a>
    
    <a href="https://geek.zshipu.com/tags/LDA/">LDA</a>
    
    <a href="https://geek.zshipu.com/tags/LSTM/">LSTM</a>
    
    <a href="https://geek.zshipu.com/tags/LSTM%E7%BD%91%E7%BB%9C/">LSTM网络</a>
    
    <a href="https://geek.zshipu.com/tags/LambdaMART/">LambdaMART</a>
    
    <a href="https://geek.zshipu.com/tags/Linux/">Linux</a>
    
    <a href="https://geek.zshipu.com/tags/LruCache/">LruCache</a>
    
    <a href="https://geek.zshipu.com/tags/Lucence/">Lucence</a>
    
    <a href="https://geek.zshipu.com/tags/MKR%E6%A8%A1%E5%9E%8B/">MKR模型</a>
    
    <a href="https://geek.zshipu.com/tags/MLflow/">MLflow</a>
    
    <a href="https://geek.zshipu.com/tags/MMoE/">MMoE</a>
    
    <a href="https://geek.zshipu.com/tags/MRR/">MRR</a>
    
    <a href="https://geek.zshipu.com/tags/Milvus/">Milvus</a>
    
    <a href="https://geek.zshipu.com/tags/MoE/">MoE</a>
    
    <a href="https://geek.zshipu.com/tags/Mock/">Mock</a>
    
    <a href="https://geek.zshipu.com/tags/Monorepo/">Monorepo</a>
    
    <a href="https://geek.zshipu.com/tags/mysql/">mysql</a>
    
    <a href="https://geek.zshipu.com/tags/NDCG/">NDCG</a>
    
    <a href="https://geek.zshipu.com/tags/NER/">NER</a>
    
    <a href="https://geek.zshipu.com/tags/NIO/">NIO</a>
    
    <a href="https://geek.zshipu.com/tags/NIPS/">NIPS</a>
    
    <a href="https://geek.zshipu.com/tags/NLP/">NLP</a>
    
    <a href="https://geek.zshipu.com/tags/Netty/">Netty</a>
    
    <a href="https://geek.zshipu.com/tags/nextjs/">nextjs</a>
    
    <a href="https://geek.zshipu.com/tags/nextjs/">nextjs</a>
    
    <a href="https://geek.zshipu.com/tags/OCR/">OCR</a>
    
    <a href="https://geek.zshipu.com/tags/OKR/">OKR</a>
    
    <a href="https://geek.zshipu.com/tags/OPPO/">OPPO</a>
    
    <a href="https://geek.zshipu.com/tags/PageRank/">PageRank</a>
    
    <a href="https://geek.zshipu.com/tags/Pinot/">Pinot</a>
    
    <a href="https://geek.zshipu.com/tags/Pulsar/">Pulsar</a>
    
    <a href="https://geek.zshipu.com/tags/Push%E7%B3%BB%E7%BB%9F/">Push系统</a>
    
    <a href="https://geek.zshipu.com/tags/QA/">Q&amp;A</a>
    
    <a href="https://geek.zshipu.com/tags/Que2Search/">Que2Search</a>
    
    <a href="https://geek.zshipu.com/tags/Query%E6%89%A9%E5%B1%95/">Query扩展</a>
    
    <a href="https://geek.zshipu.com/tags/Query%E7%90%86%E8%A7%A3/">Query理解</a>
    
    <a href="https://geek.zshipu.com/tags/R-Tree/">R-Tree</a>
    
    <a href="https://geek.zshipu.com/tags/ROC/">ROC</a>
    
    <a href="https://geek.zshipu.com/tags/RTree/">RTree</a>
    
    <a href="https://geek.zshipu.com/tags/reactjs/">reactjs</a>
    
    <a href="https://geek.zshipu.com/tags/RippleNet/">RippleNet</a>
    
    <a href="https://geek.zshipu.com/tags/RocketMQ/">RocketMQ</a>
    
    <a href="https://geek.zshipu.com/tags/SHAP/">SHAP</a>
    
    <a href="https://geek.zshipu.com/tags/SIGAI/">SIGAI</a>
    
    <a href="https://geek.zshipu.com/tags/SVM/">SVM</a>
    
    <a href="https://geek.zshipu.com/tags/Serverless/">Serverless</a>
    
    <a href="https://geek.zshipu.com/tags/SimCLR/">SimCLR</a>
    
    <a href="https://geek.zshipu.com/tags/Softmax/">Softmax</a>
    
    <a href="https://geek.zshipu.com/tags/Stage/">Stage</a>
    
    <a href="https://geek.zshipu.com/tags/TFServing/">TFServing</a>
    
    <a href="https://geek.zshipu.com/tags/TensorFlow/">TensorFlow</a>
    
    <a href="https://geek.zshipu.com/tags/Topk/">Topk</a>
    
    <a href="https://geek.zshipu.com/tags/Transformer/">Transformer</a>
    
    <a href="https://geek.zshipu.com/tags/TurboSearch/">TurboSearch</a>
    
    <a href="https://geek.zshipu.com/tags/Typora/">Typora</a>
    
    <a href="https://geek.zshipu.com/tags/WebRTC/">WebRTC</a>
    
    <a href="https://geek.zshipu.com/tags/WideDeep/">Wide&amp;Deep</a>
    
    <a href="https://geek.zshipu.com/tags/Word2vec/">Word2vec</a>
    
    <a href="https://geek.zshipu.com/tags/XDL/">XDL</a>
    
    <a href="https://geek.zshipu.com/tags/XDeepFM/">XDeepFM</a>
    
    <a href="https://geek.zshipu.com/tags/XGBoost/">XGBoost</a>
    
    <a href="https://geek.zshipu.com/tags/XLNet/">XLNet</a>
    
    <a href="https://geek.zshipu.com/tags/Yoo%E8%A7%86%E9%A2%91/">Yoo视频</a>
    
    <a href="https://geek.zshipu.com/tags/YoshuaBengio/">YoshuaBengio</a>
    
    <a href="https://geek.zshipu.com/tags/ZeroSearch/">ZeroSearch</a>
    
    <a href="https://geek.zshipu.com/tags/Zookeeper/">Zookeeper</a>
    
    <a href="https://geek.zshipu.com/tags/abtest/">abtest</a>
    
    <a href="https://geek.zshipu.com/tags/android/">android</a>
    
    <a href="https://geek.zshipu.com/tags/apache/">apache</a>
    
    <a href="https://geek.zshipu.com/tags/apollo/">apollo</a>
    
    <a href="https://geek.zshipu.com/tags/boosting/">boosting</a>
    
    <a href="https://geek.zshipu.com/tags/checkpoint/">checkpoint</a>
    
    <a href="https://geek.zshipu.com/tags/css/">css</a>
    
    <a href="https://geek.zshipu.com/tags/cto/">cto</a>
    
    <a href="https://geek.zshipu.com/tags/elasticsearch/">elasticsearch</a>
    
    <a href="https://geek.zshipu.com/tags/flutter/">flutter</a>
    
    <a href="https://geek.zshipu.com/tags/game/">game</a>
    
    <a href="https://geek.zshipu.com/tags/github/">github</a>
    
    <a href="https://geek.zshipu.com/tags/gitlab/">gitlab</a>
    
    <a href="https://geek.zshipu.com/tags/go/">go</a>
    
    <a href="https://geek.zshipu.com/tags/golang/">golang</a>
    
    <a href="https://geek.zshipu.com/tags/graphql/">graphql</a>
    
    <a href="https://geek.zshipu.com/tags/hadoop/">hadoop</a>
    
    <a href="https://geek.zshipu.com/tags/java/">java</a>
    
    <a href="https://geek.zshipu.com/tags/jdbc/">jdbc</a>
    
    <a href="https://geek.zshipu.com/tags/js/">js</a>
    
    <a href="https://geek.zshipu.com/tags/kafka/">kafka</a>
    
    <a href="https://geek.zshipu.com/tags/lab/">lab</a>
    
    <a href="https://geek.zshipu.com/tags/linUCB%E6%96%B9%E6%B3%95/">linUCB方法</a>
    
    <a href="https://geek.zshipu.com/tags/lucene/">lucene</a>
    
    <a href="https://geek.zshipu.com/tags/mybatis/">mybatis</a>
    
    <a href="https://geek.zshipu.com/tags/mysql/">mysql</a>
    
    <a href="https://geek.zshipu.com/tags/nexp/">nexp</a>
    
    <a href="https://geek.zshipu.com/tags/nextjs/">nextjs</a>
    
    <a href="https://geek.zshipu.com/tags/nifi/">nifi</a>
    
    <a href="https://geek.zshipu.com/tags/node2vec/">node2vec</a>
    
    <a href="https://geek.zshipu.com/tags/nodejs/">nodejs</a>
    
    <a href="https://geek.zshipu.com/tags/npm/">npm</a>
    
    <a href="https://geek.zshipu.com/tags/olap/">olap</a>
    
    <a href="https://geek.zshipu.com/tags/one-hot/">one-hot</a>
    
    <a href="https://geek.zshipu.com/tags/oss/">oss</a>
    
    <a href="https://geek.zshipu.com/tags/python/">python</a>
    
    <a href="https://geek.zshipu.com/tags/pytorch/">pytorch</a>
    
    <a href="https://geek.zshipu.com/tags/query%E7%BA%A0%E9%94%99/">query纠错</a>
    
    <a href="https://geek.zshipu.com/tags/react/">react</a>
    
    <a href="https://geek.zshipu.com/tags/reactjs/">reactjs</a>
    
    <a href="https://geek.zshipu.com/tags/reactor/">reactor</a>
    
    <a href="https://geek.zshipu.com/tags/redis/">redis</a>
    
    <a href="https://geek.zshipu.com/tags/region/">region</a>
    
    <a href="https://geek.zshipu.com/tags/rpc/">rpc</a>
    
    <a href="https://geek.zshipu.com/tags/scala/">scala</a>
    
    <a href="https://geek.zshipu.com/tags/select/">select</a>
    
    <a href="https://geek.zshipu.com/tags/sharding/">sharding</a>
    
    <a href="https://geek.zshipu.com/tags/skleam/">skleam</a>
    
    <a href="https://geek.zshipu.com/tags/solr/">solr</a>
    
    <a href="https://geek.zshipu.com/tags/spark/">spark</a>
    
    <a href="https://geek.zshipu.com/tags/sqllit/">sqllit</a>
    
    <a href="https://geek.zshipu.com/tags/storm/">storm</a>
    
    <a href="https://geek.zshipu.com/tags/storybook/">storybook</a>
    
    <a href="https://geek.zshipu.com/tags/tailwind/">tailwind</a>
    
    <a href="https://geek.zshipu.com/tags/trace/">trace</a>
    
    <a href="https://geek.zshipu.com/tags/vivo/">vivo</a>
    
    <a href="https://geek.zshipu.com/tags/vuejs/">vuejs</a>
    
    <a href="https://geek.zshipu.com/tags/web/">web</a>
    
    <a href="https://geek.zshipu.com/tags/web3/">web3</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%B8%80%E8%87%B4%E6%80%A7/">一致性</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%B8%91%E5%B0%8F%E9%B8%AD%E5%AE%9A%E7%90%86/">丑小鸭定理</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%B8%9A%E5%8A%A1/">业务</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%B8%9A%E5%8A%A1%E7%BA%BF/">业务线</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90/">个性化推荐</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%B5%B7%E6%8A%A5/">个性化海报</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%B8%AD%E5%85%B3%E6%9D%91/">中关村</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D/">中文分词</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%B8%AD%E6%96%87%E7%BA%A0%E9%94%99/">中文纠错</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%B8%BB%E9%A2%98%E5%BB%BA%E6%A8%A1/">主题建模</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%B9%A6%E7%B1%8D/">书籍</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%BA%8B%E4%BB%B6%E9%A9%B1%E5%8A%A8%E6%9E%B6%E6%9E%84/">事件驱动架构</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%BA%8B%E5%8A%A1/">事务</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%BA%9A%E9%A9%AC%E9%80%8A/">亚马逊</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/">交叉验证</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%BA%BA%E6%9C%BA%E9%97%AE%E7%AD%94/">人机问答</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%BE%9B%E5%BA%94%E9%93%BE/">供应链</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/">依存句法分析</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/">信息检索</a>
    
    <a href="https://geek.zshipu.com/tags/%E4%BF%A1%E6%81%AF%E6%B5%81%E6%8E%A8%E8%8D%90/">信息流推荐</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95/">倒排索引</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%80%92%E6%8E%92%E8%A1%A8/">倒排表</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/">假设检验</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%85%A8%E6%96%87%E7%B4%A2%E5%BC%95/">全文索引</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%85%A8%E6%B0%91K%E6%AD%8C/">全民K歌</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%85%A8%E9%93%BE%E8%B7%AF%E5%8E%8B%E6%B5%8B/">全链路压测</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%85%AC%E5%B9%B3%E9%94%81/">公平锁</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%85%B3%E7%B3%BB/">关系</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%85%B4%E8%B6%A3/">兴趣</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%86%85%E5%AE%B9%E6%8C%96%E6%8E%98/">内容挖掘</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%86%85%E5%AE%B9%E7%90%86%E8%A7%A3/">内容理解</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%86%B7%E5%90%AF%E5%8A%A8/">冷启动</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%87%BA%E8%BD%A8/">出轨</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%88%86%E5%B1%82%E5%AE%9E%E9%AA%8C/">分层实验</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/">分布式事务</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/">分布式系统</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/">分布式锁</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%88%86%E8%AF%8D/">分词</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%88%9B%E4%B8%9A/">创业</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8A%A0%E6%9D%83%E8%9E%8D%E5%90%88/">加权融合</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8C%97%E4%BA%AC/">北京</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8C%BA%E5%9D%97/">区块</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/">区块链</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8D%8F%E5%90%8C%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/">协同记忆网络</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/">协同过滤</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8D%8F%E6%96%B9%E5%B7%AE/">协方差</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/">单元测试</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8D%9A%E5%A3%AB/">博士</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8D%9A%E5%A3%ABoffer/">博士offer</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8E%8B%E6%B5%8B/">压测</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8E%9F%E5%88%99/">原则</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B/">双塔模型</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8F%8D%E4%BD%9C%E5%BC%8A/">反作弊</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8F%8D%E6%AC%BA%E8%AF%88/">反欺诈</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8F%8D%E8%84%86%E5%BC%B1/">反脆弱</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8F%98%E9%87%8F/">变量</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8F%AC%E5%9B%9E/">召回</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8F%AC%E5%9B%9E%E7%8E%87/">召回率</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/">可观测性</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/">可解释性</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%90%88%E7%BA%A6/">合约</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%90%8E%E5%8E%82%E6%9D%91/">后厂村</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%90%91%E9%87%8F%E5%8F%AC%E5%9B%9E/">向量召回</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2/">向量检索</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%90%91%E9%87%8F%E7%B4%A2%E5%BC%95/">向量索引</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/">吴恩达</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/">命名实体识别</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%93%8D%E5%BA%94%E5%BC%8F%E7%BC%96%E7%A8%8B/">响应式编程</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%95%86%E6%B1%A4%E7%A7%91%E6%8A%80/">商汤科技</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/">回归模型</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%9B%A0%E6%9E%9C%E5%88%86%E6%9E%90/">因果分析</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/">图像检索</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/">图像识别</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93/">图数据库</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%9B%BE%E7%81%B5%E5%B9%B3%E5%8F%B0/">图灵平台</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%9B%BE%E7%89%87%E7%BF%BB%E8%AF%91/">图片翻译</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%9B%BE%E8%AE%A1%E7%AE%97/">图计算</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%9C%A8%E7%BA%BF%E5%AD%A6%E4%B9%A0/">在线学习</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%9D%90%E6%A0%87%E5%9B%9E%E5%BD%92/">坐标回归</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%A2%9E%E9%87%8F%E5%AD%A6%E4%B9%A0/">增量学习</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%A4%9A%E5%A4%9A/">多多</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/">多模态</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96/">多目标优化</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%A4%B4%E6%9D%A1/">头条</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80/">奥卡姆剃刀</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%AD%A6%E4%B9%A0/">学习</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/">学习资料</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90/">学习资源</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%AD%A6%E4%BC%9A%E6%8F%90%E9%97%AE/">学会提问</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/">实体识别</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%AE%9E%E6%97%B6%E6%8E%A8%E8%8D%90/">实时推荐</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/">实时数仓</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE/">实时数据</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%AE%9E%E6%97%B6%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86/">实时日志收集</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/">实时计算</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%AE%9E%E9%AA%8C%E5%B9%B3%E5%8F%B0/">实验平台</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%AE%B9%E7%81%BE%E4%BD%93%E7%B3%BB%E5%BB%BA%E8%AE%BE/">容灾体系建设</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">对比学习</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/">对话系统</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%B0%8F%E5%9F%8E%E5%B8%82/">小城市</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%B0%8F%E7%B1%B3/">小米</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%B0%8F%E7%B1%B3%E6%90%9C%E7%B4%A2/">小米搜索</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%B0%8F%E7%BE%A4%E6%95%88%E5%BA%94/">小群效应</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%B7%A5%E4%BD%9C/">工作</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/">布隆过滤器</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%B8%AE%E5%B8%AE/">帮帮</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%B9%B4%E8%BD%BB%E4%BA%BA/">年轻人</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%B9%BF%E5%91%8A/">广告</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%B9%BF%E5%91%8A%E7%B3%BB%E7%BB%9F/">广告系统</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/">序列标注</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%BB%BA%E6%A8%A1%E8%B0%83%E5%8F%82/">建模调参</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%BC%80%E6%BA%90%E6%95%B0%E6%8D%AE%E9%9B%86/">开源数据集</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/">开源项目</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/">异常检测</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%BC%82%E6%AD%A5IO/">异步IO</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%BC%A0%E5%98%89%E4%BD%B3/">张嘉佳</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%BD%92%E4%B8%80%E5%8C%96/">归一化</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%BE%85%E5%88%86%E7%B1%BB/">待分类</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/">微服务</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%BE%AE%E8%BD%AFEXP/">微软EXP</a>
    
    <a href="https://geek.zshipu.com/tags/%E5%BE%AE%E8%BD%AF%E4%BA%9A%E6%B4%B2%E7%A0%94%E7%A9%B6%E9%99%A2/">微软亚洲研究院</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%80%9D%E7%BB%B4/">思维</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/">性能优化</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/">情感分析</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%84%8F%E5%9B%BE%E8%AF%86%E5%88%AB/">意图识别</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%88%BF%E7%A7%9F/">房租</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%8A%80%E6%9C%AF/">技术</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%8B%86%E5%88%86/">拆分</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%8B%9B%E8%81%98/">招聘</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%8B%BC%E5%A4%9A%E5%A4%9A/">拼多多</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98/">持续交付</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/">持续集成</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/">损失函数</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%8E%92%E5%BA%8F/">排序</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B/">排序模型</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%8E%92%E9%98%9F/">排队</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%8E%A8%E7%90%86%E7%B3%BB%E7%BB%9F/">推理系统</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%8E%A8%E8%8D%90/">推荐</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%8E%A8%E8%8D%90%E7%90%86%E7%94%B1/">推荐理由</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%8E%A8%E9%80%81%E5%B9%B3%E5%8F%B0/">推送平台</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%8F%90%E9%97%AE%E7%9A%84%E6%99%BA%E6%85%A7/">提问的智慧</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%90%9C%E7%B4%A2/">搜索</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%90%9C%E7%B4%A2%E5%B9%BF%E5%91%8A/">搜索广告</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/">搜索引擎</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%90%9C%E7%B4%A2%E6%8E%92%E5%BA%8F/">搜索排序</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%90%9C%E7%B4%A2%E6%9E%B6%E6%9E%84/">搜索架构</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F/">支持向量</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">支持向量机</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%95%B0%E4%BB%93%E4%BD%93%E7%B3%BB/">数仓体系</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%95%B0%E4%BB%93%E5%B9%B3%E5%8F%B0/">数仓平台</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0/">数据中台</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/">数据仓库</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">数据分析</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0/">数据分析平台</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/">数据同步</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0/">数据平台</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%87%E6%A0%87/">数据指标</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/">数据挖掘</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%95%B0%E6%8D%AE%E6%B2%BB%E7%90%86/">数据治理</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%95%B0%E6%8D%AE%E6%B9%96/">数据湖</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/">数据科学</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/">数据集</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8/">数据驱动</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB/">文字识别</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%96%87%E6%91%98/">文摘</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/">文本情感分类</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/">文本挖掘</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%96%87%E6%9C%AC%E7%BA%A0%E9%94%99/">文本纠错</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E5%BE%81/">文本表征</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90/">新闻推荐</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/">方法论</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%97%A5%E5%BF%97%E6%9E%B6%E6%9E%84/">日志架构</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%97%A5%E5%BF%97%E6%A3%80%E7%B4%A2/">日志检索</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%97%B6%E5%BA%8F%E7%89%B9%E5%BE%81%E6%8C%96%E6%8E%98/">时序特征挖掘</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%99%BA%E6%85%A7%E7%89%A9%E6%B5%81/">智慧物流</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6/">智能合约</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/">智能客服</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%99%BA%E8%83%BD%E7%89%A9%E6%B5%81/">智能物流</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%99%BA%E8%83%BD%E8%AF%AD%E9%9F%B3/">智能语音</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94/">智能问答</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%99%BA%E8%83%BD%E9%A2%84%E8%AD%A6/">智能预警</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%9C%8D%E5%8A%A1/">服务</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%88%B1%E5%A5%BD%E8%80%85/">机器学习爱好者</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98/">机器学习面试题</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/">机器翻译</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/">机器视觉</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB/">机器阅读</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/">条件随机场</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%9E%B6%E6%9E%84/">架构</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%9E%B6%E6%9E%84%E5%B8%88/">架构师</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%A0%87%E7%AD%BE/">标签</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91/">标签平滑</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%A0%87%E7%AD%BE%E8%AF%86%E5%88%AB/">标签识别</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%A0%87%E7%AD%BE%E9%80%89%E6%8B%A9/">标签选择</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%A0%A1%E6%8B%9B/">校招</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%A0%B7%E6%9C%AC/">样本</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%A3%80%E7%B4%A2%E5%BC%95%E6%93%8E/">检索引擎</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%A7%BD%E4%BD%8D%E8%AF%86%E5%88%AB/">槽位识别</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%A8%A1%E5%9E%8B%E5%89%AA%E6%9E%9D/">模型剪枝</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/">模型压缩</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/">模型融合</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/">模型评估</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/">模型部署</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%A8%A1%E5%9E%8B%E9%A2%84%E4%BC%B0/">模型预估</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D/">模式匹配</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/">模式识别</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%AD%A3%E5%88%99%E5%8C%96/">正则化</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">注意力机制</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%B4%8B%E7%A0%81%E5%A4%B4/">洋码头</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%B5%81%E6%89%B9%E4%B8%80%E4%BD%93/">流批一体</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/">消息队列</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%B7%B1%E5%BA%A6/">深度</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%B7%B1%E5%BA%A6%E5%85%B4%E8%B6%A3%E7%BD%91%E7%BB%9C/">深度兴趣网络</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%B7%B1%E5%BA%A6%E6%A0%91%E5%8C%B9%E9%85%8D/">深度树匹配</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%B7%B1%E5%BA%A6%E6%A0%91%E6%A3%80%E7%B4%A2/">深度树检索</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%B7%B7%E6%8E%92/">混排</a>
    
    <a href="https://geek.zshipu.com/tags/%E6%B7%B7%E6%B2%8C%E5%B7%A5%E7%A8%8B/">混沌工程</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%81%AB%E7%84%B0%E5%9B%BE/">火焰图</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%83%AD%E7%82%B9%E6%8C%96%E6%8E%98/">热点挖掘</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%86%94%E6%96%AD%E9%99%8D%E7%BA%A7/">熔断降级</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%88%AC%E8%99%AB/">爬虫</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%88%B1%E5%A5%87%E8%89%BA/">爱奇艺</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%89%9B%E9%A1%BF-%E8%8E%B1%E5%B8%83%E5%B0%BC%E8%8C%A8/">牛顿-莱布尼茨</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%89%A9%E6%B5%81/">物流</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%89%B9%E5%BE%81%E5%B9%B3%E5%8F%B0/">特征平台</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%89%B9%E5%BE%81%E7%B3%BB%E7%BB%9F/">特征系统</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%8C%9C%E4%BD%A0%E5%96%9C%E6%AC%A2/">猜你喜欢</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%94%A8%E6%88%B7%E5%BB%BA%E6%A8%A1/">用户建模</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/">用户画像</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%94%B5%E5%95%86%E6%90%9C%E7%B4%A2/">电商搜索</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%99%BD%E5%85%94/">白兔</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%99%BE%E5%BA%A6/">百度</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/">相关系数</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%9C%9F%E8%AF%9D/">真话</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%9F%A2%E9%87%8F%E8%AF%AD%E4%B9%89/">矢量语义</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%9F%A5%E4%B9%8E%E6%9E%B6%E6%9E%84/">知乎架构</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/">知识图谱</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA/">知识增强</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">知识蒸馏</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%9F%AD%E6%96%87%E6%9C%AC%E8%A7%A3%E6%9E%90/">短文本解析</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%9F%AD%E8%A7%86%E9%A2%91/">短视频</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%9F%AD%E8%AF%AD%E6%8A%BD%E5%8F%96/">短语抽取</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%9F%AD%E8%AF%AD%E6%8C%96%E6%8E%98/">短语挖掘</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%A2%A7%E6%A1%82%E5%9B%AD/">碧桂园</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%A5%9E%E9%A9%AC%E6%90%9C%E7%B4%A2/">神马搜索</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%A6%BB%E7%BA%BF%E8%AE%A1%E7%AE%97/">离线计算</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%A7%92%E6%9D%80%E6%9E%B6%E6%9E%84/">秒杀架构</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F/">秒杀系统</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/">程序人生</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%A8%8B%E5%BA%8F%E5%91%98/">程序员</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%A8%B3%E5%AE%9A%E6%80%A7%E8%A7%84%E8%8C%83/">稳定性规范</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%A9%BA%E9%97%B4%E7%B4%A2%E5%BC%95/">空间索引</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/">窗口函数</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%AB%AF%E4%B8%8A%E6%99%BA%E8%83%BD/">端上智能</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%AB%AF%E6%99%BA%E8%83%BD/">端智能</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%AE%97%E6%B3%95/">算法</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%B1%BB%E5%8D%8F%E5%90%8C%E8%AE%AD%E7%BB%83/">类协同训练</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%B1%BB%E7%9B%AE%E8%AF%86%E5%88%AB/">类目识别</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%B2%97%E6%8E%92/">粗排</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%B4%A2%E5%BC%95/">索引</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%BA%BF%E7%A8%8B/">线程</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%BA%BF%E7%A8%8B%E6%B1%A0/">线程池</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%BC%93%E5%AD%98/">缓存</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%BD%91%E7%BB%9C%E5%9B%BE/">网络图</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%BD%AE%E4%BF%A1%E5%BA%A6/">置信度</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%BE%8E%E5%9B%A2/">美团</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%BE%8E%E5%9B%A2%E5%A4%A7%E8%84%91/">美团大脑</a>
    
    <a href="https://geek.zshipu.com/tags/%E7%BE%8E%E5%9B%A2%E7%82%B9%E8%AF%84/">美团点评</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%81%8C%E5%9C%BA/">职场</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/">联邦学习</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%85%BE%E8%AE%AF%E6%8A%80%E6%9C%AF/">腾讯技术</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%85%BE%E8%AE%AF%E9%9F%B3%E4%B9%90/">腾讯音乐</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/">自动化测试</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/">自动驾驶</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%89%B2%E6%83%85%E8%AF%86%E5%88%AB/">色情识别</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%8A%B1%E6%A4%92%E7%9B%B4%E6%92%AD/">花椒直播</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%8B%9E%E8%B0%B7/">苞谷</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%9A%82%E8%9A%81%E9%87%91%E6%9C%8D/">蚂蚁金服</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%A7%84%E5%88%99%E5%B9%B3%E5%8F%B0/">规则平台</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%A7%84%E5%88%99%E5%BC%95%E6%93%8E/">规则引擎</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%A7%86%E9%A2%91%E6%8E%A8%E8%8D%90/">视频推荐</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/">计算广告</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AE%A4%E7%9F%A5/">认知</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AE%A8%E8%AE%BA%E5%8C%BA/">讨论区</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AE%B0%E5%BF%86%E5%BB%BA%E6%A8%A1/">记忆建模</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/">记忆网络</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AE%BA%E6%96%87/">论文</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/">评价指标</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87/">评测指标</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/">词向量</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AF%8D%E5%B5%8C%E5%85%A5/">词嵌入</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AF%8D%E6%9D%83%E9%87%8D/">词权重</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/">语义分割</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D/">语义匹配</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AF%AD%E4%B9%89%E6%A3%80%E7%B4%A2/">语义检索</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8B/">语义模型</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AF%AD%E4%B9%89%E7%90%86%E8%A7%A3/">语义理解</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">语言模型</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AF%AD%E9%9F%B3%E5%86%85%E5%AE%B9%E8%AF%86%E5%88%AB/">语音内容识别</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/">语音识别</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%B0%B7%E6%AD%8C%E9%9D%A2%E8%AF%95/">谷歌面试</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%92%E5%BA%8F/">贝叶斯个性化排序</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%B4%9D%E5%A3%B3%E6%89%BE%E6%88%BF/">贝壳找房</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%B4%9D%E5%A3%B3%E6%99%BA%E6%90%9C/">贝壳智搜</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%B4%A7%E5%B8%81%E5%8C%96/">货币化</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%B5%B7%E6%AD%A5/">起步</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%B6%8B%E5%8A%BF%E7%A7%91%E6%8A%80/">趋势科技</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92/">路径规划</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%BD%AF%E5%AE%9E%E5%8A%9B/">软实力</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/">边缘计算</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%BE%BE%E6%91%A9%E9%99%A2/">达摩院</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/">迁移学习</a>
    
    <a href="https://geek.zshipu.com/tags/%E8%BF%87%E6%8B%9F%E5%90%88/">过拟合</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/">逻辑回归</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%80%BB%E8%BE%91%E6%80%9D%E7%BB%B4/">逻辑思维</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%87%87%E8%B4%AD/">采购</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%87%8D%E5%8F%A0%E5%AE%9E%E9%AA%8C%E6%A1%86%E6%9E%B6/">重叠实验框架</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%87%8D%E6%8E%92%E5%BA%8F/">重排序</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%87%8D%E6%9E%84/">重构</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%87%91%E8%9E%8D/">金融</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%93%B6%E6%B1%A4%E5%8C%99/">银汤匙</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%93%BE%E8%A1%A8/">链表</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%93%BE%E8%A1%A8%E6%B1%82%E4%BA%A4%E9%9B%86/">链表求交集</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%98%BF%E9%87%8C/">阿里</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%98%BF%E9%87%8C%E4%BA%91/">阿里云</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%98%BF%E9%87%8C%E5%A6%88%E5%A6%88/">阿里妈妈</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%98%BF%E9%87%8C%E5%B0%8F%E8%9C%9C/">阿里小蜜</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%98%BF%E9%87%8C%E8%BE%BE%E6%91%A9%E9%99%A2/">阿里达摩院</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%99%88%E8%96%87/">陈薇</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%99%8C%E9%99%8C/">陌陌</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%99%8D%E7%BA%AC%E6%89%93%E5%87%BB/">降纬打击</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F/">随机变量</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%9B%B6%E6%8B%B7%E8%B4%9D/">零拷贝</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%9D%A2%E7%BB%8F/">面经</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%9D%A2%E8%AF%95/">面试</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/">项目管理</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%A2%84%E4%BC%B0%E5%BC%95%E6%93%8E/">预估引擎</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83/">预训练</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%A2%86%E5%9F%9F%E8%AE%BE%E8%AE%A1/">领域设计</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8/">领域驱动</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%A3%8E%E6%8E%A7/">风控</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%A3%8E%E6%8E%A7%E7%B3%BB%E7%BB%9F/">风控系统</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/">高可用</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/">高并发</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%AB%98%E6%96%AF%E7%83%AD%E5%9B%BE/">高斯热图</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%AB%98%E7%BB%B4%E6%95%B0%E6%8D%AE%E7%B4%A2%E5%BC%95/">高维数据索引</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%BB%84%E5%B3%A5/">黄峥</a>
    
    <a href="https://geek.zshipu.com/tags/%E9%BB%91%E7%9B%92%E6%A8%A1%E5%9E%8B/">黑盒模型</a>
    
</div>
    </section>

    
<section class="widget">
    <h3 class="widget-title">友情链接</h3>
    <ul class="widget-list">
        
        <li>
            <a target="_blank" href="https://blog.zshipu.com//" title="知识铺的博客">知识铺的博客</a>
        </li>
        
    </ul>
</section>


    <section class="widget">
        <h3 class="widget-title">其它</h3>
        <ul class="widget-list">
            <li><a href="https://geek.zshipu.com/index.xml">文章 RSS</a></li>
        </ul>
    </section>
</div>
            </div>
        </div>
    </div>
</body>

</html>